{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Gzg8SopX8EWH"
      },
      "source": [
        "# Vicuna LLaMa 13B LoRa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQyg3MygxKCW",
        "outputId": "e7d17924-c63c-4e1f-ebff-0e103337f15b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
            "CUDA SETUP: Detected CUDA version 121\n",
            "CUDA SETUP: Loading binary /home/maksim/gitrepo/vicuna_lora/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda121.so...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/maksim/gitrepo/vicuna_lora/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('1'), PosixPath('0')}\n",
            "  warn(msg)\n",
            "/home/maksim/gitrepo/vicuna_lora/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('local/maksim-desktop'), PosixPath('@/tmp/.ICE-unix/3172,unix/maksim-desktop')}\n",
            "  warn(msg)\n",
            "/home/maksim/gitrepo/vicuna_lora/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
            "  warn(msg)\n",
            "/home/maksim/gitrepo/vicuna_lora/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/etc/xdg/xdg-ubuntu')}\n",
            "  warn(msg)\n",
            "/home/maksim/gitrepo/vicuna_lora/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n"
          ]
        }
      ],
      "source": [
        "# To avoid CUDA OOM error when getting state_dict\n",
        "# !pip uninstall bitsandbytes\n",
        "# !pip install bitsandbytes==0.37.2\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import gc\n",
        "import re\n",
        "import random\n",
        "import warnings\n",
        "import pickle\n",
        "from tqdm.auto import tqdm\n",
        "from typing import Tuple\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pymorphy2\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "from datasets import load_dataset\n",
        "from transformers import LlamaForCausalLM, LlamaTokenizer, GenerationConfig\n",
        "from transformers import TrainerCallback\n",
        "from peft import prepare_model_for_int8_training, LoraConfig, get_peft_model, PeftConfig, PeftModel, AdaLoraConfig, TaskType\n",
        "\n",
        "os.environ['WANDB_NOTEBOOK_NAME'] = \"Vicuna_13B_CHGK.ipynb\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_model(source_model_name_or_path: str):\n",
        "    \"\"\"\n",
        "    Returns a transformers model and tokenizer\n",
        "    \"\"\"\n",
        "    model = LlamaForCausalLM.from_pretrained(\n",
        "        source_model_name_or_path,\n",
        "        load_in_8bit=True,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "    tokenizer = LlamaTokenizer.from_pretrained(\n",
        "        source_model_name_or_path,\n",
        "        use_fast=False\n",
        "    )\n",
        "    tokenizer.pad_token_id = 0  # unk. we want this to be different from the eos token\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "def generate_prompt(data_point):\n",
        "    # result = f\"\"\"### Вопрос: {data_point[\"Вопрос\"]} ### \"\"\"\n",
        "    # if data_point.get(\"Комментарий\") is not None:\n",
        "    #     result += f\"\"\"Комментарий: {data_point[\"Комментарий\"]};\"\"\"\n",
        "    # result += f\"\"\"Ответ: {data_point[\"Ответ\"][:45]}\"\"\"\n",
        "    return f\"\"\"Ответь на вопрос викторины. Вопрос: {data_point[\"Question\"]} Ответ: {data_point[\"Answer\"][:45]}\"\"\"\n",
        "    \n",
        "def generate_prompt_infer(data_point):\n",
        "    return f\"\"\"Ответь на вопрос викторины. Вопрос:{data_point[\"Question\"]} Ответ: \"\"\"\n",
        "\n",
        "def generate(quest, model, tokenizer, temperature=0.0, top_p=0.9, repetition_penalty=1.4, max_new_tokens=64, cutoff_len=512):\n",
        "    inputs = tokenizer(\n",
        "        generate_prompt_infer(quest),\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=cutoff_len,\n",
        "    )\n",
        "    input_ids = inputs[\"input_ids\"].cuda()\n",
        "\n",
        "    generation_config = GenerationConfig(\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        repetition_penalty=repetition_penalty\n",
        "    )\n",
        "    model.eval()\n",
        "    model.config.use_cache = True\n",
        "    with torch.no_grad():\n",
        "        generation_output = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            generation_config=generation_config,\n",
        "            return_dict_in_generate=True,\n",
        "            output_scores=True,\n",
        "            max_new_tokens=max_new_tokens\n",
        "        )\n",
        "    output = tokenizer.decode(generation_output.sequences[0], skip_special_tokens=True)\n",
        "    output = output.split(' Ответ: ')\n",
        "    if len(output) > 1:\n",
        "        return output[1]\n",
        "    else:\n",
        "        return \"\"\n",
        "    \n",
        "def generate_beams(quest, model, tokenizer, temperature=0.5, top_p=0.9, repetition_penalty=1.3, max_new_tokens=64, cutoff_len=512):\n",
        "    inputs = tokenizer(\n",
        "        generate_prompt_infer(quest),\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=cutoff_len,\n",
        "    )\n",
        "    input_ids = inputs[\"input_ids\"].cuda()\n",
        "\n",
        "    generation_config = GenerationConfig(\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        sampling=True,\n",
        "        repetition_penalty=repetition_penalty,\n",
        "        num_beams=6,\n",
        "        num_beam_groups=2,\n",
        "        min_length=2,\n",
        "        use_cache=True,\n",
        "        # diversity_penalty=0.17,\n",
        "        # encoder_repetition_penalty=1.5\n",
        "    )\n",
        "    model.eval()\n",
        "    model.config.use_cache = True\n",
        "    with torch.no_grad():\n",
        "        generation_output = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            generation_config=generation_config,\n",
        "            return_dict_in_generate=True,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            num_return_sequences=1,\n",
        "            # no_repeat_ngram_size=2,\n",
        "            remove_invalid_values=True,\n",
        "        )\n",
        "    output = tokenizer.decode(generation_output.sequences[0], skip_special_tokens=True)\n",
        "    output = output.split(' Ответ: ')\n",
        "    if len(output) > 1:\n",
        "        return output[1]\n",
        "    else:\n",
        "        return \"\"\n",
        "\n",
        "def print_dict(data: dict):\n",
        "    if data is not None:\n",
        "        for key, value in data.items():\n",
        "            print(f'{key}: {value}')\n",
        "\n",
        "def is_acceptable_quiz(quiz: dict) -> bool:\n",
        "    if len(re.findall(\"\\(pic: \\d*.\\w{1,3}\\)\", quiz['Question'])) > 0:\n",
        "        return False\n",
        "    if len(re.findall(\"<раздатка>\", quiz['Question'])) > 0:\n",
        "        return False\n",
        "    return True\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "j7qZqe0YxG2c"
      },
      "source": [
        "## Fine Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "76iZTtGJy26x"
      },
      "outputs": [],
      "source": [
        "SOURCE_MODEL = \"vicuna-13b/\"\n",
        "\n",
        "MICRO_BATCH_SIZE = 6  \n",
        "BATCH_SIZE = 256\n",
        "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n",
        "EPOCHS = 1  # paper uses 3\n",
        "LEARNING_RATE = 2e-4  # from the original paper with batch size 128 lr=2e-5\n",
        "WARMUP_STEPS = 10\n",
        "CUTOFF_LEN = 256  \n",
        "CUTOFF_LEN_TEST = 160\n",
        "MAX_ANSWER_LENGTH = 25\n",
        "LORA_R = 128\n",
        "LORA_ALPHA = 24\n",
        "LORA_DROPOUT = 0.0\n",
        "TEST_SIZE = 0.004\n",
        "DEFAULT_LORA_0 = \"checkpoints/vicuna-13b_LoRA_default_0\"\n",
        "SAVED_LORA = \"checkpoints/vicuna-13b_checkpoint_LoRA_0.04504\"\n",
        "INPUT_DIR = 'chgk_baza'\n",
        "DO_COMPILE = False\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"down_proj\"],\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    bias=\"lora_only\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "8c0b2ebd6c63498fadb16e1c29c0ee03",
            "b067fd4b35724cbc8791ff4548fe46da",
            "c5e728cf48664d3a83348fad1605983a",
            "404c6bfeb19f477a8ffec70706087e6c",
            "0f43b4ac5ceb4b8a80fab468915e3097",
            "7f04e58038e64192a0ad1058c2111675",
            "5d2cea68374346afa3c49e04aa4e9481",
            "dfa5e05bd7974455931c639766acfd9a",
            "8bb67fbe2a3e4357b5c72b6c2e1b37db",
            "b92c8a8222bc40248724de6f6a22f264",
            "3a2119497387439ab42cf52bfe7e0fe6"
          ]
        },
        "id": "tI5Ta-gQy56c",
        "outputId": "dfe60805-78ef-4c8a-9e07-90204a2daf78"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e218a391bf64d44b06bd7923422bfcc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model, tokenizer = get_model(SOURCE_MODEL)\n",
        "model = prepare_model_for_int8_training(model)\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.save_pretrained(DEFAULT_LORA_0)\n",
        "# model.load_adapter(model_id=SAVED_LORA, adapter_name='default')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 290449, Test:1167 \n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b4f0b157fccf431a93a449cebf469b1a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/290449 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d527f3aca2564a3f9a31b00de65d7f7f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1167 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 290449, Test:1014\n"
          ]
        }
      ],
      "source": [
        "with open(os.path.join(INPUT_DIR, 'quest_clean_list.pkl'), 'rb') as f:\n",
        "    quest_clean_list = pickle.load(f)\n",
        "\n",
        "# Train test split\n",
        "data_train_raw, data_test_raw = train_test_split(quest_clean_list, test_size=TEST_SIZE, random_state=42)\n",
        "print(f\"Train: {len(data_train_raw)}, Test:{len(data_test_raw)}\", \"\\n\")\n",
        "\n",
        "raw_data = dict()\n",
        "data = dict()\n",
        "train_data = []\n",
        "test_data = []\n",
        "\n",
        "for quiz in tqdm(data_train_raw):\n",
        "    text = generate_prompt(quiz)\n",
        "    tokens = tokenizer(text, truncation=False)\n",
        "    if len(tokens[\"input_ids\"]) <= CUTOFF_LEN:\n",
        "        train_data.append(tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            max_length=CUTOFF_LEN,\n",
        "            padding=\"max_length\",\n",
        "        ))\n",
        "for quiz in tqdm(data_test_raw):\n",
        "    text = generate_prompt_infer(quiz)\n",
        "    tokens = tokenizer(text, truncation=False)\n",
        "    if len(tokens[\"input_ids\"]) <= CUTOFF_LEN_TEST:\n",
        "        test_data.append(tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            max_length=CUTOFF_LEN_TEST,\n",
        "            padding=\"max_length\",\n",
        "        ))\n",
        "\n",
        "data['train'] = train_data\n",
        "data['test'] = test_data\n",
        "raw_data['test'] = data_test_raw\n",
        "raw_data['train'] = data_train_raw\n",
        "del train_data, test_data, data_train_raw, data_test_raw\n",
        "\n",
        "print(f\"Train: {len(data['train'])}, Test:{len(data['test'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "def preprocess_metrics(text: str) -> str:\n",
        "    replace_list = [\"\\.\", \",\", \"\\?\", \"!\", \"\\n\", '\"', \"/\", \";\", \":\", \"1\\)\", \"2\\)\", \"3\\)\", \"4\\)\", \"\\(\", \"-\", \"\\)\"]\n",
        "    text = re.sub('|'.join(replace_list), ' ', text)\n",
        "    text = text.lower()\n",
        "    text = ' '.join([morph.parse(x)[0].normal_form for x in text.split()])\n",
        "    return text\n",
        "\n",
        "def compute_metrics():\n",
        "    model.save_pretrained(SAVED_LORA)\n",
        "    model.eval()\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    model.config.use_cache = True\n",
        "    predictions = []\n",
        "    references = []\n",
        "    for quest in tqdm(raw_data['test']):\n",
        "        predictions.append(generate(quest, model, tokenizer, repetition_penalty=1.2, max_new_tokens=MAX_ANSWER_LENGTH))\n",
        "        references.append(quest[\"Answer\"][:45])\n",
        "    references = [preprocess_metrics(x) for x in references]\n",
        "    predictions = [preprocess_metrics(x) for x in predictions]\n",
        "    model.train()\n",
        "    model.config.use_cache = False\n",
        "\n",
        "    f2_list = []\n",
        "    pr_list = []\n",
        "    rec_list = []\n",
        "\n",
        "    for pred, ref in zip(predictions, references):\n",
        "        ref_set = set(ref.split())\n",
        "        pred_set = set(pred.split())\n",
        "        intersect = ref_set & pred_set\n",
        "        pr = len(intersect) / (len(pred_set) + 1e-5)\n",
        "        rec = len(intersect) / (len(ref_set) + 1e-5)\n",
        "        f2 = 5 * pr * rec / (4 * pr + rec + 1e-5)\n",
        "        f2_list.append(f2)\n",
        "        pr_list.append(pr)\n",
        "        rec_list.append(rec)\n",
        "\n",
        "    precision = np.array(pr_list).mean()\n",
        "    recall = np.array(rec_list).mean()\n",
        "    f2 = np.array(f2_list).mean()\n",
        "\n",
        "    del predictions, references\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    print(f\"Presision={precision:.5f}\\nRecall={recall:.5f}\\nF2={f2:.5f}\")\n",
        "    return {'precision': precision, 'recall': recall, 'F2':f2}\n",
        "\n",
        "def calc_metrics(predictions: list, references: list, print_best=False) -> Tuple[float, float, float]:\n",
        "    f2_list = []\n",
        "    pr_list = []\n",
        "    rec_list = []\n",
        "\n",
        "    for pred, ref in zip(predictions, references):\n",
        "        ref_set = set(preprocess_metrics(ref).split())\n",
        "        pred_set = set(preprocess_metrics(pred).split())\n",
        "        intersect = ref_set & pred_set\n",
        "        pr = len(intersect) / (len(pred_set) + 1e-5)\n",
        "        rec = len(intersect) / (len(ref_set) + 1e-5)\n",
        "        f2 = 5 * pr * rec / (4 * pr + rec + 1e-5)\n",
        "        f2_list.append(f2)\n",
        "        pr_list.append(pr)\n",
        "        rec_list.append(rec)\n",
        "        if print_best and pr > 0:\n",
        "            print()\n",
        "            print(ref)\n",
        "            print(\"GPT finetuned:\", pred, '\\n')\n",
        "\n",
        "    precision = np.array(pr_list).mean()\n",
        "    recall = np.array(rec_list).mean()\n",
        "    f2 = np.array(f2_list).mean()\n",
        "    return precision, recall, f2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "23ae4d80693d4e9ca9fa84ec04582fe0",
            "083dc60725f34ef698730f26c9086dcb",
            "f0452141712c4d8ea206b70fb5df6d75",
            "acdf3dc9176645108458e29f66a35708",
            "303c086f27154250ace62e34947d8ca6",
            "0de70903e0134f0e83dc678521357e69",
            "15017ff480f24f62946bda435cd51102",
            "352276c412234087821381d50daa52e9",
            "2975c4c32ac949c9ba538a8e5f99c441",
            "62c966d78b12433b9258a023602d709e",
            "8972cf3fb361445db3d798b734055d78"
          ]
        },
        "id": "kWP89TPIwRkK",
        "outputId": "c700fc6a-f0af-4c72-a29d-50dec6e56db5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 351272960 || all params: 13367137280 || trainable%: 2.6278847343445553\n"
          ]
        }
      ],
      "source": [
        "class MetricsCallback(TrainerCallback):\n",
        "    def on_save(self, args, state, control, **kwargs):\n",
        "\n",
        "        model.save_pretrained(SAVED_LORA)\n",
        "\n",
        "        model.eval()\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        model.config.use_cache = True\n",
        "        predictions = []\n",
        "        references = []\n",
        "        for quest in tqdm(raw_data['test']):\n",
        "            predictions.append(generate(quest, model, tokenizer, repetition_penalty=1.4, max_new_tokens=MAX_ANSWER_LENGTH))\n",
        "            references.append(quest[\"Answer\"][:45])\n",
        "        model.train()\n",
        "        model.config.use_cache = False\n",
        "\n",
        "        precision, recall, f2 = calc_metrics(predictions, references, print_best=False)\n",
        "\n",
        "        del predictions, references\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        print(f\"Presision={precision:.5f}\\nRecall={recall:.5f}\\nF2={f2:.5f}\")\n",
        "    \n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=data['train'],\n",
        "    eval_dataset=data['test'],\n",
        "    callbacks=[MetricsCallback()],\n",
        "    # compute_metrics=compute_metrics,\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=MICRO_BATCH_SIZE,\n",
        "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "        warmup_steps=WARMUP_STEPS,\n",
        "        num_train_epochs=EPOCHS,\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        bf16=True,\n",
        "        seed=44,\n",
        "        dataloader_num_workers=2,\n",
        "        logging_steps=1,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_accumulation_steps=1,\n",
        "        eval_steps=50, # 50\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=100,\n",
        "        save_safetensors=True,\n",
        "        output_dir=\"lora-alpaca\",\n",
        "        save_total_limit=2,        \n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "model.config.use_cache = False\n",
        "model.print_trainable_parameters()\n",
        "model.gradient_checkpointing_enable()\n",
        "if DO_COMPILE and torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
        "    model = torch.compile(model)\n",
        "    print(\"Model compiled!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpolushinm\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.15.4 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.2"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/maksim/gitrepo/vicuna_lora/wandb/run-20230611_165109-xtoxv2cn</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/polushinm/huggingface/runs/xtoxv2cn' target=\"_blank\">earnest-shape-158</a></strong> to <a href='https://wandb.ai/polushinm/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/polushinm/huggingface' target=\"_blank\">https://wandb.ai/polushinm/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/polushinm/huggingface/runs/xtoxv2cn' target=\"_blank\">https://wandb.ai/polushinm/huggingface/runs/xtoxv2cn</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2917ed69f4dd4d5ebc3cab1c420d556c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1071 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>warnings.filterwarnings(action=<span style=\"color: #808000; text-decoration-color: #808000\">\"ignore\"</span>, message=<span style=\"color: #808000; text-decoration-color: #808000\">\"MatMul8bitLt: inputs will be cast from</span>    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>warnings.filterwarnings(action=<span style=\"color: #808000; text-decoration-color: #808000\">\"ignore\"</span>, message=<span style=\"color: #808000; text-decoration-color: #808000\">\"This implementation of AdamW\"</span>)            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 3 trainer.train(resume_from_checkpoint=<span style=\"color: #808000; text-decoration-color: #808000\">'lora-alpaca/checkpoint-200'</span>)                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span>model.save_pretrained(SAVED_LORA)                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/maksim/gitrepo/vicuna_lora/lib/python3.10/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1662</span> in    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">train</span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1659 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>inner_training_loop = find_executable_batch_size(                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1660 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._inner_training_loop, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._train_batch_size, args.auto_find_batch_size  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1661 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1662 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> inner_training_loop(                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1663 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>args=args,                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1664 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>resume_from_checkpoint=resume_from_checkpoint,                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1665 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>trial=trial,                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/maksim/gitrepo/vicuna_lora/lib/python3.10/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1929</span> in    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_inner_training_loop</span>                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1926 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> model.no_sync():                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1927 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   </span>tr_loss_step = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.training_step(model, inputs)                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1928 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1929 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>tr_loss_step = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.training_step(model, inputs)                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1930 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1931 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> (                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1932 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>args.logging_nan_inf_filter                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/maksim/gitrepo/vicuna_lora/lib/python3.10/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2717</span> in    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">training_step</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2714 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># loss gets scaled under gradient_accumulation_steps in deepspeed</span>             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2715 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>loss = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.deepspeed.backward(loss)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2716 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2717 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>loss.backward()                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2718 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2719 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> loss.detach()                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2720 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/maksim/gitrepo/vicuna_lora/lib/python3.10/site-packages/torch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_tensor.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">487</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 484 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>create_graph=create_graph,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 485 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>inputs=inputs,                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 486 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>)                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 487 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>torch.autograd.backward(                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 488 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, gradient, retain_graph, create_graph, inputs=inputs                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 489 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 490 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/maksim/gitrepo/vicuna_lora/lib/python3.10/site-packages/torch/autograd/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">200</span> in  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>                                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">197 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># The reason we repeat same the comment below is that</span>                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">198 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># some Python versions print out the first line of a multi-line function</span>               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">199 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># calls in the traceback and some print out the last line</span>                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>200 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>Variable._execution_engine.run_backward(  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to run the bac</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">201 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>tensors, grad_tensors_, retain_graph, create_graph, inputs,                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">202 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>allow_unreachable=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>, accumulate_grad=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to ru</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">203 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
              "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m3\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 1 \u001b[0mwarnings.filterwarnings(action=\u001b[33m\"\u001b[0m\u001b[33mignore\u001b[0m\u001b[33m\"\u001b[0m, message=\u001b[33m\"\u001b[0m\u001b[33mMatMul8bitLt: inputs will be cast from\u001b[0m    \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 2 \u001b[0mwarnings.filterwarnings(action=\u001b[33m\"\u001b[0m\u001b[33mignore\u001b[0m\u001b[33m\"\u001b[0m, message=\u001b[33m\"\u001b[0m\u001b[33mThis implementation of AdamW\u001b[0m\u001b[33m\"\u001b[0m)            \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 3 trainer.train(resume_from_checkpoint=\u001b[33m'\u001b[0m\u001b[33mlora-alpaca/checkpoint-200\u001b[0m\u001b[33m'\u001b[0m)                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 4 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 5 \u001b[0mmodel.save_pretrained(SAVED_LORA)                                                           \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 6 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/home/maksim/gitrepo/vicuna_lora/lib/python3.10/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m1662\u001b[0m in    \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[92mtrain\u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1659 \u001b[0m\u001b[2m│   │   \u001b[0minner_training_loop = find_executable_batch_size(                                 \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1660 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._inner_training_loop, \u001b[96mself\u001b[0m._train_batch_size, args.auto_find_batch_size  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1661 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1662 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m inner_training_loop(                                                       \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1663 \u001b[0m\u001b[2m│   │   │   \u001b[0margs=args,                                                                    \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1664 \u001b[0m\u001b[2m│   │   │   \u001b[0mresume_from_checkpoint=resume_from_checkpoint,                                \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1665 \u001b[0m\u001b[2m│   │   │   \u001b[0mtrial=trial,                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/home/maksim/gitrepo/vicuna_lora/lib/python3.10/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m1929\u001b[0m in    \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[92m_inner_training_loop\u001b[0m                                                                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1926 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mwith\u001b[0m model.no_sync():                                                 \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1927 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0mtr_loss_step = \u001b[96mself\u001b[0m.training_step(model, inputs)                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1928 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                     \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1929 \u001b[2m│   │   │   │   │   \u001b[0mtr_loss_step = \u001b[96mself\u001b[0m.training_step(model, inputs)                      \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1930 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m                                                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1931 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m (                                                                      \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1932 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0margs.logging_nan_inf_filter                                           \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/home/maksim/gitrepo/vicuna_lora/lib/python3.10/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m2717\u001b[0m in    \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[92mtraining_step\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m2714 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# loss gets scaled under gradient_accumulation_steps in deepspeed\u001b[0m             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m2715 \u001b[0m\u001b[2m│   │   │   \u001b[0mloss = \u001b[96mself\u001b[0m.deepspeed.backward(loss)                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m2716 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2717 \u001b[2m│   │   │   \u001b[0mloss.backward()                                                               \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m2718 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m2719 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m loss.detach()                                                              \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m2720 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/home/maksim/gitrepo/vicuna_lora/lib/python3.10/site-packages/torch/\u001b[0m\u001b[1;33m_tensor.py\u001b[0m:\u001b[94m487\u001b[0m in \u001b[92mbackward\u001b[0m   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 484 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mcreate_graph=create_graph,                                                \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 485 \u001b[0m\u001b[2m│   │   │   │   \u001b[0minputs=inputs,                                                            \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 486 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 487 \u001b[2m│   │   \u001b[0mtorch.autograd.backward(                                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 488 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m, gradient, retain_graph, create_graph, inputs=inputs                     \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 489 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 490 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/home/maksim/gitrepo/vicuna_lora/lib/python3.10/site-packages/torch/autograd/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m200\u001b[0m in  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[92mbackward\u001b[0m                                                                                         \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m197 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# The reason we repeat same the comment below is that\u001b[0m                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m198 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# some Python versions print out the first line of a multi-line function\u001b[0m               \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m199 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# calls in the traceback and some print out the last line\u001b[0m                              \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m200 \u001b[2m│   \u001b[0mVariable._execution_engine.run_backward(  \u001b[2m# Calls into the C++ engine to run the bac\u001b[0m   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m201 \u001b[0m\u001b[2m│   │   \u001b[0mtensors, grad_tensors_, retain_graph, create_graph, inputs,                        \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m202 \u001b[0m\u001b[2m│   │   \u001b[0mallow_unreachable=\u001b[94mTrue\u001b[0m, accumulate_grad=\u001b[94mTrue\u001b[0m)  \u001b[2m# Calls into the C++ engine to ru\u001b[0m   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m203 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
              "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
              "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "warnings.filterwarnings(action=\"ignore\", message=\"MatMul8bitLt: inputs will be cast from\")\n",
        "warnings.filterwarnings(action=\"ignore\", message=\"This implementation of AdamW\")\n",
        "trainer.train(resume_from_checkpoint='lora-alpaca/checkpoint-200')\n",
        "\n",
        "model.save_pretrained(SAVED_LORA)\n",
        "\n",
        "# Presision=0.02992\n",
        "# Recall=0.03854\n",
        "# F2=0.03434"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model.save_pretrained('checkpoints/vicuna-13b_checkpoint_LoRA_chk200')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.load_adapter(model_id=\"checkpoints/vicuna-13b_checkpoint_LoRA_chk200\", adapter_name='default')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 1.9868e-03,  1.2566e-04,  8.0444e-03,  ...,  7.2476e-04,\n",
              "         -2.1179e-03, -4.8381e-03],\n",
              "        [ 3.3036e-03,  4.3476e-03,  1.4770e-03,  ..., -4.0143e-03,\n",
              "          6.8942e-03, -5.1013e-04],\n",
              "        [ 8.6272e-03,  9.3042e-03,  2.8657e-03,  ..., -6.9144e-03,\n",
              "          1.4964e-02, -9.9965e-05],\n",
              "        ...,\n",
              "        [ 2.6522e-03,  4.5705e-03,  5.5359e-03,  ..., -5.0041e-03,\n",
              "          9.6899e-03, -2.1749e-03],\n",
              "        [ 1.4776e-02,  1.5573e-02,  1.2891e-02,  ..., -6.4296e-03,\n",
              "          1.4631e-02, -8.8199e-03],\n",
              "        [-6.2615e-03, -8.4163e-03, -1.1182e-02,  ...,  5.6907e-03,\n",
              "         -6.5634e-03,  5.8730e-03]], device='cuda:0')"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# model.load_adapter(model_id=SAVED_LORA, adapter_name='default')\n",
        "# model.tie_weights()\n",
        "model.state_dict()['base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CqfJCzzPxvaQ"
      },
      "source": [
        "## Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model, tokenizer = get_model(SOURCE_MODEL)\n",
        "# model = get_peft_model(model, peft_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJYi3pdNemBn",
        "outputId": "eba9ffb7-a581-4945-86e6-fe32da948f09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dba6ad498c544adc88075135d558f917",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1167 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Question: ТЯЖЕЛАЯ ТЕМА  1. Так расшифровывается аббревиатура КВ в названии советского тяжелого танка  2. Так называется вода, в молекулу которой вместо протия входит дейтерий  3. Название этого химического элемента происходит от греческого слова \"тяжелый\"  4. Борис Громов сменил на посту губернатора Московской области именно его  5. Этот боксер снялся в главной роли в художественном фильме \"Тяжелые перчатки\"\n",
            "Answer: 1. Клим Ворошилов  2. Тяжелая вода  3. Барий  4. Анатолий Тяжлов  5. Ласло Папп\n",
            "GPT finetuned: 1. КВ (Клим Ворошилов)  2. Деутерий  3\n",
            "\n",
            "\n",
            "Question: [Ведущему: неявно выделить голосом слово \"определенный\".]  Тренер \"Арсенала\" Герберт Чэпмен хотел, чтобы клуб был всюду на первом месте. Поэтому в определенный момент отдал распоряжение навсегда убрать ЭТО. Назовите ЭТО.\n",
            "Answer: Определенный артикль The в названии клуба.  Определенный артикль, Артикль The, The.\n",
            "Comment: Для того чтобы \"The Arsenal\" находился на первом месте не только по результатам, но и по алфавиту, тренер велел убрать из названия определенный артикль.\n",
            "GPT finetuned: 1-е место.  Первое место, первое место в таблИцкОм рядк\n",
            "\n",
            "\n",
            "Question: В жизни бывают иногда странные совпадения. На Хайгейтском кладбище Лондона по соседству расположены могилы двух философов, которые при жизни были непримиримыми антагонистами. К тому же это соседство невольно заставляет многих посетителей кладбища вспомнить известную сеть. Напишите фамилии этих философов.\n",
            "Answer: Маркс и Спенсер.\n",
            "Comment: Могилы Карла Маркса и Герберта Спенсера находятся рядом, что невольно напоминает о знаменитой сети универмагов \"Маркс энд Спенсер\".\n",
            "GPT finetuned:  Маркс и Энгельс.   Маркс и Энгельс.   Маркс и Эн\n",
            "\n",
            "\n",
            "Question: Римляне называли друзей ворами, причём ворующими самое дорогое. Что именно?\n",
            "Answer: Время.\n",
            "GPT finetuned:  Времена. Римляне называли друзей временщиками.  ВрЕменн\n",
            "\n",
            "\n",
            "Question: Удивительно, но фактически это самый большой в Европе заповедник для жизни таких редких животных, как медведь, выдра, барсук, ондатра, рысь, олень, зубр, лошадь Пржевальского. Рассматривается возможность поселения росомахи и амурского тигра. Какое событие привело к возникновению этого заповедника?\n",
            "Answer: Авария на Чернобыльской АЭС.  \n",
            "Comment: Речь идет о зоне отчуждения Чернобыльской АЭС, огромной территории (4000 кв. км), в которой прекращена хозяйственная деятельность человека и отсутствует население. Кроме нескольких сильно загрязненных участков, радиация практически не влияет на экосистему радиационной зоны. Организмы диких животных сами справляются с повышенным фоном, с химическим загрязнением территории и другими негативными факторами.\n",
            "GPT finetuned:  Чернобыльская катастрофа. Заповедник - Полесскiй лiс\n",
            "\n",
            "\n",
            "Question: Почему в Уэльсе в XIX в. не получил распространения марксизм?\n",
            "Answer: Валлийский язык очень сложный, и не нашлось переводчиков Капитала на него.\n",
            "GPT finetuned:  Марксистские идеи не прижились в Уэльсе, поскольк  В Уэль\n",
            "\n",
            "\n",
            "Question: Дуплет.  1. Первое в СССР баночное пиво было выпущено к Московской Олимпиаде. На банке были изображены гербы десятка советских городов. Как называлось это пиво?  2. Один туристический маршрут проходит по территории Новгородской, Ленинградской и Псковской областей. Как называется этот маршрут?\n",
            "Answer: 1. Золотое кольцо России.  2. Серебряное кольцо России.\n",
            "Comment: 1. И олимпийская символика, и русская история.\n",
            "GPT finetuned: 1. Золотая лихорадка.  2. Золотое кольцо.   1.\n",
            "\n",
            "\n",
            "Question: В основе конструкции одной из разновидностей ЭТОГО УСТРОЙСТВА лежит атомайзер, внутри которого расположена нихромовая спираль. Спираль, раскаляясь, превращает ароматизированную жидкость в густой пар. Один картридж для ЭТОГО УСТРОЙСТВА заменяет примерно 5-7... Чего?\n",
            "Answer: Сигарет.\n",
            "Comment: Описано устройство т.н. электронной сигареты.\n",
            "GPT finetuned: 5-7 пачек сигарет.  Пачек сигарет.  Сигарет\n",
            "\n",
            "\n",
            "Question: Статья Анны ТолстОвой посвящена моментам в жизни знаменитого скульптора, не отраженным в биографических трудах о нем. Какими двумя словами она называется?\n",
            "Answer: Неизвестный Неизвестный.\n",
            "Comment: Неизвестные факты об Эрнсте Неизвестном.\n",
            "GPT finetuned: 10 неизвестных.  Неизвестные 10.  10 неизвестнЫХ\n",
            "\n",
            "\n",
            "Question: Хазары.  1. Эта религия была господствующей в Хазарском каганате.  2. Таково имя верховного хазарского божества доиудаистического периода истории.  3. Из этого рода происходили все хазарские каганы.  4. В письме к этому испанскому иудею каган Иосиф изложил историю Хазарии.  5. Этот человек в начале IX века произвел в Хазарии государственный переворот и превратил иудаизм в государственную религию каганата.\n",
            "Answer: 1. Иудаизм.  2. Тенгри-хан.  3. Ашина.  4. Хасдай Ибн-Шафрут.  5. Обадия.\n",
            "GPT finetuned: 1. Иудаизм.  2. Тенгри.  3.\n",
            "\n",
            "\n",
            "Question: На австралийском гербе в качестве щитодержателей изображены кенгуру и страус эму. И не только потому, что они специфически австралийские животные - мало ли эндемиков в Австралии, - а еще и потому, что они обладают неким биомеханическим свойством, которое делает их использование в геральдике очень даже уместным. Можно сказать, что они символизируют известный девиз времен II Мировой войны. Так что это за свойство, присущее и кенгуру, и страусу эму?\n",
            "Answer: Их ноги устроены так, что они не могут сделать ни шагу назад.\n",
            "GPT finetuned: Они бегают быстрее, чем могут ле\n",
            "\n",
            "\n",
            "Question: Эти три слова знакомы нам по трем разным европейским языкам, в которых они обозначают одно и то же. У нас же французский вариант ассоциируется, скорее, с живописью, немецкий - с топографией, английский же вариант с печатанием документов. Назовите любое из этих слов.\n",
            "Answer: Landscape, ландшафт, пейзаж.\n",
            "GPT finetuned: ландшафт, пейзаж, панорама. \n",
            "\n",
            "\n",
            "Question: В 1737 году появилось \"Описание и чертежи вновь изобретённой машины для буксировки судов в гавани и из гаваней, портов и рек, против ветра, прилива или в штилевую погоду\". Что это за машина?\n",
            "Answer: Пароход.\n",
            "GPT finetuned:  Пароход.  Паровая машина.  Паровая лодка.  ПаролОд.\n",
            "\n",
            "\n",
            "Question: ОНА часто становилась причиной смерти людей. Так, в прошлом веке из-за неё погибли несколько великих русских. В 70-х годах нашего века \"ОНА\" стала первой ступенькой к успеху очень известного американца. А один француз, если верить русской версии известного произведения, занимался ЕЮ, вопреки запретам свыше, в среднем один раз в сутки. А как звали его лучших друзей?\n",
            "Answer: ОНА - дуэль, на дуэли погибли Пушкин и Лермонтов, первый фильм Спилберга назывался Дуэль, француз - Арамис: Хоть Бог и запретил дуэли, но к шпаге чувствую талант, Дерусь семь раз я на неделе, хоть, право, я не дуэлянт. А друзья - соответственно - Портос, д'Артаньян, Атос.\n",
            "GPT finetuned: 22 кота.  Кот Матроскин, Кот Матроскин и его 22 \n",
            "\n",
            "\n",
            "Question: Первыми документально подтверждёнными поселенцами были финикийцы. Позднее их сменили греки, основавшие колонию и соорудившие на уединённой скале храм Геракла Пустынника. А чем славится это место сейчас?\n",
            "Answer: пустынник - монах - Монако - азартными играми\n",
            "GPT finetuned: нимбами. Речь идет о Монако. \n",
            "\n",
            "\n",
            "Question: Героиня одной из миниатюр Вересаева рассказывала о том, что они с мужем объяснились в любви \"совсем как Кити и Левин в \"Анне Карениной\". Только букв было написано меньше. В ответ на три буквы будущего мужа она написала четыре. Напишите и вы эти четыре буквы.\n",
            "Answer: и я в л.  Разумеется, вариант я В. т.(тоже) л. тоже принимается.\n",
            "Comment: \"Он всего три буквы написал: \"я В. л.\" А я ему в ответ четыре: \"и я В. л.\"\n",
            "GPT finetuned: лява.  ЛАВА.  Лава.  ЛАВА.  Люба.  Л\n",
            "\n",
            "\n",
            "Question: Иллюстрируя свою мысль, Брайс ДемОри располагает три палочки с маршмЭллоу возле костра и сетует, что даже маршмэллоу, находящаяся на идеальном расстоянии от огня, всё равно наполовину подгорит, а наполовину не пропечется, потому что всегда повернута к огню одной стороной. Ответьте точно: что Демори сравнивает с маршмэллоу?\n",
            "Answer: Экзопланеты.  Экзопланеты звезды Траппист-1, планеты, вращающиеся вокруг Траппист-1, экзопланеты в зоне обитаемости.\n",
            "Comment: Брайс Демори - ученый из Кембриджа, охотник за экзопланетами - рассказывает таким образом о трех экзопланетах, вращающихся вокруг звезды Траппист-1, которые теоретически находятся в зоне обитаемости. При этом планета, находящаяся на идеальном расстоянии от звезды, скорее всего, под действием приливных сил вращается вокруг своей оси с той же скоростью, что и вокруг светила, поэтому повернута к нему всегда одной стороной.\n",
            "GPT finetuned: 60-центовую монету.  Монету достоинством в 60 цент\n",
            "\n",
            "\n",
            "Question: Каждый знает, что на Олимпийские игры в Древней Греции женщины не допускались. Это факт достоверный. Но в древнегреческих играх был один вид состязаний, олимпийским чемпионом в котором могла стать женщина.   В каком виде состязаний древнегреческих олимпиад победителем могла быть объявлена женщина?\n",
            "Answer: В гонках колесниц победителем объявлялся не ведущий колесницу, а хозяин коней, которым могла быть и женщина.\n",
            "GPT finetuned:  В панкратионе - борьбе без правил. \n",
            "\n",
            "\n",
            "Question: Назовите любимое занятие персонажа одного мультсериала по прозвищу \"Луи 88 пальцев\".\n",
            "Answer: Игра на пианино.  Игра на рояле, игра на фортепиано.\n",
            "Comment: Диапазон большинства фортепиано составляет 88 полутонов от ля субконтроктавы до до 5-й октавы.\n",
            "GPT finetuned: 88-километровый бег.  Бег на 88 километров, 8\n",
            "\n",
            "\n",
            "Question: С какой целью советские гимнастки накануне важных соревнований нередко посещали сауну?\n",
            "Answer: Чтобы сбросить лишний вес.\n",
            "GPT finetuned:  Для того, чтобы похудеть.\n",
            "\n",
            "\n",
            "Question: Стремление к симметрии в средневековом искусстве было столь велико, что иногда приводило к нарушению канонов. Так, в Суассоне была удалена фигура одного из волхвов. А как в Парижском соборе поступили со сценой, на которой святой Мартин дарит свой плащ нищему?\n",
            "Answer: Добавили второго нищего с другой стороны.  Добавили второго нищего ().\n",
            "Comment: Мартин стал дарить свой плащ двум нищим.\n",
            "GPT finetuned:  Сняли фигуру святого Мартина.  По смыслу с укр.  Пр.\n",
            "\n",
            "\n",
            "Question: Семейная пара из японского города Кавасаки отправилась перед поездкой закусить в привокзальный ресторан. Решив, что брать в ресторан пятимесячного ребенка неловко, они спрятали спящую девочку в самое надежное и безопасное, по их мнению, место на вокзале. Куда же?\n",
            "Answer: В камеру хранения.\n",
            "GPT finetuned:  В велосипедное колесо.  В колесо велосипеда.  В \n",
            "\n",
            "\n",
            "Question: В произведении, события в котором разворачиваются во время Первой мировой войны, штабс-ротмистр рассматривает карту. Каждая страна на ней украшена изображением государственного герба. Какую поговорку произносит герой, переведя взгляд с герба одной страны на другую?\n",
            "Answer: Одна голова хорошо, а две лучше.\n",
            "Comment: \"Над Российской империей и над Германией парили орлы, похожие, как родные братья <...> Он любовно погладил отечественную геральдическую птицу по короне\".\n",
            "GPT finetuned: 2+2=5.  Два плюс два равно пять.  Два и два \n",
            "\n",
            "\n",
            "Question: Когда ученые исследовали природу северного сияния, они пришли к выводу, что оно вызывается прохождением электрического тока через разреженные газы. В ходе экспериментов были выявлены два газа, дающие оптимальный эффект, что вскоре нашло широкое применение на практике. Что это за газы?\n",
            "Answer: Неон и аргон.\n",
            "Comment: Реклама.\n",
            "GPT finetuned: 2 газа - азот и кислород. Оптимальный эффект лжес\n",
            "\n",
            "\n",
            "Question: В каждой из них - по два, в каждом из этих двух - в среднем по три. Раньше для изготовления этого был нужен половозрелый слон. Назовите двух животных, без которых применение этого не обходится сейчас.\n",
            "Answer: Рыба и козел.\n",
            "GPT finetuned: овца и баран. Речь идет о шерсти.  \n",
            "\n",
            "\n",
            "Question: Первое официальное поздравление по поводу 3000-летия Иерусалим получил:  а) из России  b) из Соединенных Штатов  c) из Новой Зеландии  d) из государства Фалястын\n",
            "Answer: c) из Новой Зеландии\n",
            "Comment: От королевы маори\n",
            "GPT finetuned:  а) из России. \n",
            "\n",
            "\n",
            "Question: Если у самурая плохое настроение и ему захотелось побранить свою жену, той следует плакать, целовать мужу ноги и молить о прощении. Но самураю рекомендуется не переусердствовать в ругани. Почему?\n",
            "Answer: Жена может решить, что она недостойна жить, и перерезать себе горло.\n",
            "GPT finetuned:  Если он будет ругать ее слишком долго, она  Вспрёт \n",
            "\n",
            "\n",
            "Question: Радиостанция \"Эхо Москвы\" проводила в 1999 году опрос с целью определения \"персон века\". В номинации \"Зарубежные кумиры века\" первое место заняли, естественно, \"Битлз\", рядом с ними заняли места: Пресли, Монро, принцесса Диана, Агата Кристи, С. Дали, Че Гевара, Жан Маре и Фрэнк Синатра. А какие два имени заняли седьмую строку по требованию современной молодежи?\n",
            "Answer: Бивис и Батхед.\n",
            "GPT finetuned: Брюс Уиллис и Арнольд Шварценеггер.\n",
            "\n",
            "\n",
            "Question: Русский религиозный философ Николай Бердяев был сторонником неограниченного суверенитета государства. Вставьте два антонима в его утверждение. \"Государство существует не для создания (пропуск), а для того, чтобы не дать превратиться жизни в (пропуск)\".\n",
            "Answer: Рая, ад.  Рай, ад.\n",
            "Comment: Религиозный же философ...\n",
            "GPT finetuned: 1) рая, 2) ада.  1) рай, 2) ад.  1)\n",
            "\n",
            "\n",
            "Question: Довольно странную пару можно встретить в некоторых странах мира. Это два мужчины: один - высокого роста и ездит на белом коне, другой - это его слуга мавр Черный Питер, одетый, как средневековый паж. За спиной у него висит мешок с подарками, а в руках - розги для непослушных детей. Кто выполняет подобные функции у нас?\n",
            "Answer: Дед Мороз и Снегурочка.\n",
            "GPT finetuned: Ёжик.  Санта-Клаус.  Дед Мороз.  Снегурочка.\n",
            "\n",
            "\n",
            "Question: Чихание - не патология, а нормальная реакция организма на раздражение верхних дыхательных путей. Почему же чихающему человеку говорят \"Будь здоров!\"?\n",
            "Answer: По древним верованиям, душа, живущая в теле человека, покидает его с последним вздохом, когда человек умирает. Но если сильно чихнуть, то можно случайно выпихнуть душу, что чревато для чихнувшего самыми плачевными последствиями.\n",
            "GPT finetuned: 1. Чтобы он перестал чихать.  2. Чтобы он не лжн\n",
            "\n",
            "\n",
            "Question: Герой фантастического романа Артура Кларка совершает путешествие. В одном из эпизодов описывается тусклое освещение и упоминается, что ПЕРВАЯ словно стала гигантской ВТОРОЙ для ВТОРОЙ. Назовите ПЕРВУЮ и ВТОРУЮ.\n",
            "Answer: Земля, Луна.\n",
            "Comment: Земля, так же как и Луна для землян, стала дополнительным светилом, отражая свет Солнца для лунных жителей.\n",
            "GPT finetuned:  Луна,  Земля.  Луна, планета.  Луна, Земля, пла\n",
            "\n",
            "\n",
            "Question: Когда персы в 490 г. до н.э. шли на Грецию, им указал Марафон как наилучшее место для высадки старый Гиппий, изгнанный сын афинского тирана Писистрата. Гиппию было предсказано, что кости его будут лежать в Аттике, и он надеялся на победу персов. Однако при высадке он закашлялся от поднятой воинами пыли, и с ним произошло нечто, после чего он понял, что предсказание уже исполнилось и Афин ему не увидать. Что же с ним случилось?\n",
            "Answer: У него выпал зуб.\n",
            "GPT finetuned: Он умер\n",
            "\n",
            "\n",
            "Question: ВАЛУЕВ В НАЧАЛЕ КАРЬЕРЫ  Преимущества силы и роста  Мне хватает, чтоб выиграть бой.  К чёрту тактику! [...  ...] с малышнёй.\n",
            "Answer: Я дерусь просто,  Потому что дерусь\n",
            "GPT finetuned:  Я буду бить, как будто -  Дядя Степа.  А нырнул \n",
            "\n",
            "\n",
            "Question: Их противостоянию уже много лет. С.М.Соловьев, близкий друг Блока, вспоминает, что в стихотворении \"Поединок\" их борьба изображалась как поединок русского царя со святым. Назовите имена обоих.\n",
            "Answer: Петр (Первый - Петербург) и Георгий (Победоносец - Москва).\n",
            "GPT finetuned: Ёжик и медведь.  Ёж и медведь.  Ёжик и мед\n",
            "\n",
            "\n",
            "Question: За финальный матч чемпионата мира 1966 года по тысяче фунтов получили даже те английские футболисты, которые просто нарисовали ИХ белой краской на своих бутсах. Игрокам сборной ФРГ этого делать, разумеется, не нужно было. Нарисуйте ИХ на бланке для ответа.\n",
            "Answer: Три параллельные наклонные полосы, как на этих бутсах: (pic: 20180244.jpg).  Любые три параллельные полосы.\n",
            "Comment: Немецкая компания \"Adidas\" [адидАс], логотип которой представляет собой три полосы, заплатила по 1000 фунтов не только английским футболистам, надевшим фирменную экипировку, но и тем, кто лишь сделал вид, что играет в бутсах, произведенных компанией. Сборная ФРГ была полностью экипирована \"Adidas\" [адидАс], поэтому немецким игрокам ничего дополнительно рисовать не нужно было.\n",
            "GPT finetuned: 0.  Зеро.  Нуль.  Любое изображение цифры 0.\n",
            "\n",
            "\n",
            "Question: На Руси из этих трех блюд первое в иное время вообще не ели, второе если и ели, то придавали ему другую форму и подавали под другим названием, например, \"папушник\". А что в обычное время не делали с третьим блюдом?\n",
            "Answer: Не красили.\n",
            "Comment: На пасху едят собственно пасху (и никогда больше), кулич и крутые крашеные яйца. В обычное время яйца не красили.\n",
            "GPT finetuned: никак его не готовили.  \n",
            "\n",
            "\n",
            "Question: Девизом Ивана Ерохина, мастера спорта по альпинизму, были слова: \"Скорость в горах - залог безопасности\". Почему, по его мнению, скоростной альпинизм безопаснее обычного?\n",
            "Answer: Меньше времени находишься в горах - зоне повышенной опасности\n",
            "Comment: меньше вероятности попасть под камнепад, встретить непогоду и т.д.\n",
            "GPT finetuned:  Скорость в горах - залог безопасности, потому  чт,  что  е\n",
            "\n",
            "\n",
            "Question: В 1934 году 24-летний Эдди Рознер стал обладателем \"Золотой трубы\" - главного приза Международного конкурса джазовых музыкантов в Италии. Молодой трубач так понравился самому Луи Армстронгу, что великий музыкант подарил подрастающему коллеге свою фотографию с надписью \"Белому Луи Армстронгу\". Какая надпись стояла на подаренном в ответ фото Рознера?\n",
            "Answer: Черному Эдди Рознеру.\n",
            "Comment: Эдди Рознер родился в Берлине в 1910 году. Сын польского эмигранта-сапожника. Закончил Берлинскую консерваторию как скрипач, но европейскую славу получил как трубач-виртуоз. Его коронный номер - игра на двух трубах.\n",
            "GPT finetuned:  Черному Эдди Рознеру.  Черному Луи Армстронгу.  Бл. \n",
            "\n",
            "\n",
            "Question: В романе МорИса Дрюона новоиспеченный монарх явился на королевский совет и по привычке СДЕЛАЛ ЭТО, хотя потом осознал, что ДЕЛАТЬ ЭТОГО и не стоило. В мультфильме 1967 года паровоз, краснея, тоже ДЕЛАЕТ ЭТО. Какие три слова в вопросе заменены на \"ДЕЛАЕТ ЭТО\"?\n",
            "Answer: Извиняется за опоздание.\n",
            "Comment: Новый король пришел на совет с опозданием и по привычке, как и в то время, когда он был принцем, извинился, но потом быстро понял, что теперь именно он и должен входить в зал последним. В мультфильме \"Паровозик из Ромашкова\" главный персонаж любовался природой в пути, из-за чего опоздал на вокзал, за что потом, краснея, извинялся.\n",
            "GPT finetuned: Ыкает из-за уха.  Икает из-за уха.  Икает из-\n",
            "\n",
            "\n",
            "Question: Инструкция по пользованию гранатой из журнала \"Сатирикон\": 1. Убедиться в наличии гранаты. 2. Выдернуть кольцо. 3. Кинуть гранату. Назовите последний пункт.\n",
            "Answer: Убедиться в отсутствии гранаты.\n",
            "GPT finetuned: 4. Убедиться в наличии гранаты. \n",
            "\n",
            "\n",
            "Question: [Ведущему: разобраться, с какой интонацией читать слова \"значительно меньше одного компонента\" - не в том смысле, что \"<1 компонента\". :-)]  Для продукции, сбываемой в Китае, компания \"Пепси\" использовала значительно меньше одного компонента и значительно больше другого компонента. Назовите оба компонента.\n",
            "Answer: Синий и красный цвет.  Синяя и красная краска (в любом порядке).\n",
            "Comment: Красные банки более по душе жителям красного Китая.\n",
            "GPT finetuned: 7UP и 7DOWN.  7UP и 7DOWN.  7UP и 7DOWN\n",
            "\n",
            "\n",
            "Question: Встретились как-то 2 мышки. Одна и спрашивает у другой: \"Ну как тебе понравился последний кинофильм?\" Что ответила ей вторая?\n",
            "Answer: Он мне гораздо больше понравился в книжном варианте.\n",
            "GPT finetuned: 100% не понравился. 100 процентов не понравилс\n",
            "\n",
            "\n",
            "Question: Во времена правителя Шихуанди при вхождении в императорский дворец в городе Чаньане многие злоумышленники были раскрыты. Из чего были сделаны ворота перед входом?\n",
            "Answer: Из магнитного железа (невиданная сила вытаскивала спрятанное оружие из-под одежды).\n",
            "GPT finetuned:  Золото.  Серебро.  Медь.  Железо.  Другие металл\n",
            "\n",
            "\n",
            "Question: По словам драматурга Эдуарда Радзинского, мужчине в жизни должно повезти с женой. А с кем, по его мнению, должно повезти писателю?\n",
            "Answer: С вдовой.\n",
            "GPT finetuned:  с редактором.  с издателем.  с издательством.  с лiт.  с\n",
            "\n",
            "\n",
            "Question: Э.и Ж.Гонкуры в своем \"Дневнике\" писали: \"История - это роман, который был; роман -...\" А как они описали такое понятие, как \"роман\"?\n",
            "Answer: Это история, какой она могла бы быть.\n",
            "GPT finetuned:  История - это роман, который был; роман - история,  котр  не было. \n",
            "\n",
            "\n",
            "Question: Рассказывая об ЭТОМ, журналист Илья Нагибин упоминает неожиданно яркий среднерусский ландшафт. По наблюдению Нагибина, ставки в ЭТОМ в целом выше, чем в рулетке, где судьба шарика зависит от слепого случая. Назовите ЭТО.\n",
            "Answer: Пейнтбол.  По слову пейнтбол.\n",
            "Comment: В этой игре судьба шарика зависит не от слепого случая, а от меткости участников.\n",
            "GPT finetuned: Ёлкино ухо.  По слову Ёлкино или Ёлкин.  Ёлкин\n",
            "\n",
            "\n",
            "Question: Впервые они были установлены на улицах Парижа в 1903 г. Их было всего девять. Они были квадратные, белые на черном фоне. Тогда считали, что они лучше будут видны издалека. Теперь их гораздо больше, они бывают различных форм и расцветок. О чем идет речь?\n",
            "Answer: О знаках дорожного движения.\n",
            "GPT finetuned:  О светофорах.  Светофоры.  Светофор.  Светогр\n",
            "\n",
            "\n",
            "Question: После обработки специальными растворами мех сушат горячим воздухом. Но ворсинки меха слипаются, ухудшая этот процесс. Что делают, чтобы избежать этого?\n",
            "Answer: Сушат мех в электростатическом поле.\n",
            "GPT finetuned:  Смазывают мех маслом. Ворсинки не слипаются,  ткн\n",
            "\n",
            "\n",
            "Question: Упоминание о первом, по сути, ИКСЕ встречается у Плутарха в эссе \"Слава Афин\". ИКС назван по имени отца Коринфа. А где проводится хронологически последний ИКС?\n",
            "Answer: В Одессе.  Черное море.\n",
            "Comment: ЭТО - марафон. У Плутарха приведена легенда о гонце, доставившем известие о победе под Марафоном. Город Марафон назван по имени Марафона, эпонима города Марафон, у которого был сын Коринф, эпоним Коринфа.\n",
            "GPT finetuned: 2004 г. в Афинах.  2004 г. в Греции.  20\n",
            "\n",
            "\n",
            "Question: В XVIII веке оба они носили одно и то же название. В 1814 году один из них его утратил, а другой, значительно уступающий первому в размерах, называется так и сейчас. Каково же современное название первого и неизменное название второго?\n",
            "Answer: Континент Австралия и остров Новая Голландия в Санкт-Петербурге.\n",
            "Comment: Голландский мореплаватель Виллем Янсзон в 1606-м году открыл новый континент (который, впрочем, долгое время считался островом) и назвал его Новой Голландией. В 1814 году Мэтью Флиндерс предложил новое название - Австралия, которое и укоренилось. Новая Голландия в Питере - небольшой остров, образуемый рекой Мойкой и двумя каналами - Крюковым и Адмиралтейским. Название острова возникло еще в начале XVIII века и связано с судостроительной специализацией этого участка города.\n",
            "GPT finetuned: 1-й и 2-й Рейхстаги.  1-й и 2-й Рей\n",
            "\n",
            "\n",
            "Question: Рабочие инструменты.  1. Вопреки своему названию, ОН не рубит, а строгает.  2. ОНА освободила Голубого щенка от цепей.  3. ЕГО название к поцелуям никакого отношения не имеет - это искажение составного немецкого слова.  4. А название ЭТОГО ИНСТРУМЕНТА происходит от французских слов, означающих \"зажим\" и \"стержень\".  5. ЭТА ДЕТАЛЬ многих рабочих инструментов и механизмов представляет собой диск, насаженный на вращающийся вал так, что ось его вращения не совпадает с геометрической осью. А еще ЭТО - амплуа многих артистов.\n",
            "Answer: 1. Рубанок.  2. Рыба-пила.  3. Лобзик (искаженное немецкое Laubsage лАубзэге).  4. Пассатижи (pince - зажим и tige - стержень).  5. Эксцентрик.\n",
            "GPT finetuned: 1. Реза\n",
            "\n",
            "\n",
            "Question: Михаил Гаспаров отмечал, что обычно ЭТО длится от 20 до 30 секунд. Сам Гаспаров гордился тем, что у него ЭТО длилось столько, сколько нужно. А вот сидящим в зале мы бы ЭТО не порекомендовали. Назовите ЭТО.\n",
            "Answer: Минута молчания.\n",
            "GPT finetuned: 10 секунд тишины.  10 секунд молчания.  10 сек\n",
            "\n",
            "\n",
            "Question: В июне 1901г. внимание жителей Англии было приковано к гонке двух клиперов, стартовавших из одного из китайских портов и взявших курс к берегам туманного Альбиона. Счастливчиком оказался капитан судна, прибывшего в устье Темзы на 6 часов раньше соперника, но не потому, что выиграл в скорости или установил мировой рекорд. Повезло капитану в другом: с ним заключили контракты на поставку определенного товара все ведущие фирмы Англии, торговавшие им. Что это за товар?\n",
            "Answer: Чай.\n",
            "GPT finetuned: Чая.  Китайского чая.  Тайваньского\n",
            "\n",
            "\n",
            "Question: Среди россиян был проведен опрос, какая русская картина самая известная. На первое место россияне поставили картину, на которой изображены шестеро млекопитающих, на втором - картина с четырьмя млекопитающими, на третьем и четвертом месте иконы, но с разницей в написании около 600 лет. Назовите эти картины в правильном порядке.\n",
            "Answer: Богатыри (три мужика и три лошади), Утро в сосновом бору, Троица Рублева и Черный квадрат - икона XX века.\n",
            "GPT finetuned: 1. Боярыня Морозова, 2. Иван Грозный и сын его 3. \n",
            "\n",
            "\n",
            "Question: Руаль Амундсен отмечал, что одной из важнейших составляющих рациона его полярной экспедиции была продукция компании \"Фрейя\". А сама эта компания, по одной из версий, изображена в книге, родители автора которой были норвежцами. Назовите эту книгу.\n",
            "Answer: Чарли и шоколадная фабрика.\n",
            "Comment: В полярных условиях нужна легкоусвояемая высококалорийная пища, особенно такая, которой можно подкрепиться на ходу. Эту роль для участников экспедиции Амундсена выполнял шоколад \"Фрейя\". По одной из версий, родившийся в Англии Роальд Даль - кстати, названный в честь Руаля Амундсена - именно фабрику \"Фрейя\" обессмертил в виде шоколадной фабрики Вилли Вонки.\n",
            "GPT finetuned: 1001 ночь.  Тысяча и одна ночь.  1001, 1\n",
            "\n",
            "\n",
            "Question: Недавно Барак Обама принял в своей резиденции обладателей Кубка Стэнли - хоккейный клуб \"Питтсбург Пингвинз\". Хоккеисты подарили президенту клубный свитер. А какой номер был на этом свитере?\n",
            "Answer: 44.\n",
            "Comment: Порядковый номер президента Барака Обамы.\n",
            "GPT finetuned: 44.  Четыредцать четыре.  44.  44-й.  \n",
            "\n",
            "\n",
            "Question: В феврале 2002 года власти одного британского города достигли соглашения с администрацией местной церкви святого Петра. Городскому совету удалось спасти от демонтажа деревянные подмостки, которые многие считают реликвией. Назовите тех двоих, чья историческая встреча произошла здесь 6 июля 1957 года.\n",
            "Answer: Джон Леннон и Пол Маккартни.\n",
            "GPT finetuned: 50-й президент США Джимми Картер и Никита Хрушчев.  В\n",
            "\n",
            "\n",
            "Question: В 1979 году Айзек Азимов выпустил книгу \"Опус 200\", а в 1984 году - книгу \"Опус 300\". Названия были даны не случайно. А что они значили?\n",
            "Answer: 200-ая и 300-ая книги соответственно.\n",
            "GPT finetuned: 200 и 300 - это номера, присвоенные этим книгам в \n",
            "\n",
            "\n",
            "Question: Не так давно среди россиян был проведен опрос на тему: \"Какой вид спорта в России можно считать национальным?\". Четвертое место по данным опроса заняло фигурное катание, третье - лыжные виды спорта, второе - футбол. А какой вид спорта, несмотря на все неудачи последних лет, оказался на первом месте?\n",
            "Answer: Хоккей.\n",
            "GPT finetuned:  Хоккей с шайбой.  Хоккей.  Хоккей на траве. \n",
            "\n",
            "\n",
            "Question: Летом 2010 года администрация городских бассейнов Вены ввела новые правила для купальщиков. Так, например, мужчинам предписывалось плавать только в плавках, а не в купальных трусах-бермудах. Назовите причину этого предписания.\n",
            "Answer: Посетители уносят с одеждой лишнюю воду.\n",
            "Comment: В условиях аномальной жары 2010 г. в европейских странах каждая капля воды была на счету.\n",
            "GPT finetuned:  По сравнению с бермудами плавки позволяют мнОгО  Меньше\n",
            "\n",
            "\n",
            "Question: Что, по мнению Коко Шанель, делает мужчина, когда действительно хочет сделать женщине подарок?\n",
            "Answer: Он женится на ней.\n",
            "GPT finetuned: ходит к ней домой.\n",
            "\n",
            "\n",
            "Question: Описание этой детективной истории мы находим в старинных литературных источниках. Камень, камыш, палка, каменное оружие, ослиная челюсть, ветка древа познания, возможно еще что-либо. Таковы различные версии относительно орудия, с помощью которого один известный человек совершил относительно другого человека преступление, предусмотренное Уголовным Кодексом Украины. Назовите это преступление и обоих персонажей.\n",
            "Answer: Убийство Авеля Каином.\n",
            "GPT finetuned: 1. Убийство, 2. Иисус Христос.  1. Убий\n",
            "\n",
            "\n",
            "Question: На самом деле этих категорий три. Третья, сравнительно мало известная, называется \"голуби\". Назовите две более известные категории.\n",
            "Answer: Совы и жаворонки.\n",
            "Comment: Категории людей по пику работоспособности.\n",
            "GPT finetuned: овсянки и воробьи.  В любом порядке.  1) овсян\n",
            "\n",
            "\n",
            "Question: Внимание, в вопросе некое слово заменено на Каценэленбоген.  Первый старше второго на 680 лет. В первом умер крупный философ, во втором - джазовый музыкант. Сюжеты второго сравнивают с хасидскими притчами. Первый получил вместо Каценэленбоген другое имя, второй взял себе вместо Каценэленбоген другую фамилию, и про старую мало кто помнит. Назовите эти имя первого и фамилию второго.\n",
            "Answer: Калининград и Аллен.\n",
            "Comment: До 1946 г. Калининград назывался Кенигсберг (основан в 1255 г.). Настоящее имя Вуди Аллена (родился в 1935 г.) - Аллен Стюарт Кенигсберг. В Кенигсберге умер И. Кант. В. Аллен - страстный поклонник джаза и кларнетист-любитель.\n",
            "GPT finetuned: Шолом-Алейхем и Иосиф Бродский.  1) Шолом-\n",
            "\n",
            "\n",
            "Question: Это животное красовалось на гербе Кристофера Хаттона, фаворита английской королевы Елизаветы I. А в 1578 году по другую сторону Атлантики оно пришло на смену пеликану. Что это за животное?\n",
            "Answer: Золотая лань. Хаттон был одним из организаторов экспедиции Фрэнсиса Дрейка и корабль Дрейка Пеликан переименовали в его честь.\n",
            "GPT finetuned: лягушка. (Пеликан - символ Франции, лягушка - лiгв\n",
            "\n",
            "\n",
            "Question: Недавно коллега, увидев на стене у клиентов карту мира, спросил: \"А чего это у них Гренландия размером с Африку?\" Автор вопроса объяснил ему, отчего так получается, но коллега возразил, что его интересует не \"отчего\", а \"для чего\"? В смысле, зачем понадобилось придумывать такой ненормальный способ построения карт? А в самом деле, для чего?\n",
            "Answer: Для прокладки курса (в проекции Меркатора искажены расстояния, но сохранены углы).\n",
            "GPT finetuned:  Для игры в шахматы.  Для игры в шашки.  Для  игр  в \n",
            "\n",
            "\n",
            "Question: Новый завет, Первое послание к коринфянам святого апостола Павла, глава 13, стихи 4-8: Она \"долготерпит, милосердствует,... не завидует,... не превозносится, не гордится, не бесчинствует, не ищет своего, не раздражается, не мыслит зла, не радуется неправде,... все покрывает, всему верит, все переносит\". Назовите ее.\n",
            "Answer: Любовь.\n",
            "GPT finetuned:  Любовь.  Любовь Божья.  Любовь Христа.  Любовь \n",
            "\n",
            "\n",
            "Question: АЛЕКСАНДР - СВОИМ ПОЛКОВОДЦАМ  Верьте, други, это не причуда.  Объясняю каждому из вас:  В рукопашной с ней придется х[...  ...] мой приказ.\n",
            "Answer: удо.  Брить ее - солдатам\n",
            "Comment: Александр Македонский приказал своим воинам гладко бриться, дабы рука врага не схватила бороду.\n",
            "GPT finetuned: ломкой,  И в бою я ее полюбил.  Ах, жаль, \n",
            "\n",
            "\n",
            "Question: Норвежские футболисты, игравшие в матчах, которые судил арбитр Пер Арне Братаас, очень редко получали карточки, даже за самые грубые нарушения. В конце концов выяснилось, что судья страдал болезнью, называемой дислексия. Так почему же он так неохотно выносил наказания?\n",
            "Answer: Чтобы лишний раз не было необходимости писать (в протоколе причину наказания).\n",
            "Comment: Дислексия вызывает трудности при чтении и правописании.\n",
            "GPT finetuned:  Он не умел читать номера на футболках игроков.  К  н  о\n",
            "\n",
            "\n",
            "Question: Рассказывают, что один канадский бизнесмен решил расширить свое дело в Венгрию. Прилетел в Будапешт, быстро изучил обстановку на месте и очень быстро смотался оттуда, предварительно послав компаньону телеграмму, одно из слов которой было \"Улетел!\". Воспроизведите весь текст телеграммы.\n",
            "Answer: Пришел! Увидел! Улетел!.\n",
            "GPT finetuned:  Улетел!  Улетел!  Улетел!  Улетел!  Улетел! \n",
            "\n",
            "\n",
            "Question: 14 октября 1997 года на территории штата Невада летчик ВВС Великобритании Энди Грин повторил то, что пятьюдесятью годами ранее сделал американский пилот Чарльз Игер. Скажите, что именно сделали оба пилота, и в чем принципиальная разница их достижений?\n",
            "Answer: Они превысили скорость звука. Игер сделал это на самолете, Грин на автомобиле.\n",
            "GPT finetuned: 14 октября 1947 года Чарльз Игер стал первым лётцом, \n",
            "\n",
            "\n",
            "Question: Этот поистине великий человек побывал почти во всех странах Европы. А последним его прибежищем стал Дом инвалидов в Париже. Кто он?\n",
            "Answer: Наполеон Бонапарт.\n",
            "GPT finetuned:  Наполеон Бонапарт.  Наполеон.  Бонапарт.  Бонап\n",
            "\n",
            "\n",
            "Question: Диалог с форума на playboy.com.ru.  Вопрос: - Что вы говорите сразу после секса?  Ответ: - \"Спокойной ночи\" или \"Проваливай\" - в зависимости...  От чего?\n",
            "Answer: В гостях отвечавший или дома.\n",
            "GPT finetuned: - \"Спокойной ночи\" или \"Проваливай\" - в зависимости...  От чего?\n",
            "\n",
            "\n",
            "Question: В этом вопросе речь пойдет о двух европейцах - мужчине и женщине. Их фамилии широко распространены - например, в США они занимают 1-е и 5-е места по популярности. Имена у них более редкие - имя мужчины лишь 69-е по популярности в США, а имя женщины и вовсе 781-е. В то же время есть сведения, что в свое время эти имена пользовались огромной популярностью. Назовите и мужчину, и женщину.\n",
            "Answer: Адам Смит и Ева Браун.\n",
            "GPT finetuned: Джон и Мэри.  Джон и Мэри Поппинс.  Джон и Мэр\n",
            "\n",
            "\n",
            "Question: 0, 1, 1, 2, 3, 5, 8... Продолжите.\n",
            "Answer: 13, 21, 34... (числа Фибоначчи).\n",
            "GPT finetuned: 13, 21, 34, 55, 89, 144... (\n",
            "\n",
            "\n",
            "Question: В 1984 г. в Китае был сделан бульон, цена которого составила 27 тысяч долларов, при этом весил он около 400 г. А на острове Мэн в 1986 г. бульон, весивший около 700 г, стоил 16 тысяч долларов. Из чего были сделаны такие дорогие бульоны?\n",
            "Answer: Из золота (это монеты специального чекана).\n",
            "Comment: \"Бульон\" - калька с английского \"bullion\" (слиток).\n",
            "GPT finetuned: 23-каратовое золото.  23-каратный золото.  Зол\n",
            "\n",
            "\n",
            "Question: Жан Антуан Гудон создал замечательную галерею скульптурных портретов философов, политиков, художников своего времени. Интересный анализ лучших портретов Ж. Гудона оставил О. Роден. Чьему скульптурному портрету посвящены следующие слова: \"Какая удивительная вещь! Это воплощенная насмешка. Глаза несколько в раскос, будто подстерегают противника. Острый нос напоминает лисицу; он весь извивается, пронюхивая всюду злоупотребления и повод к насмешке... А рот - какое совершенство! С обеих сторон он окаймлен ироническими морщинками. Того и гляди, он отпустит какой-нибудь сарказм\"?\n",
            "Answer: портрету Вольтера.\n",
            "GPT finetuned: Вольтера.  Фи\n",
            "\n",
            "\n",
            "Question: По мнению газеты \"Красная Бурда\" существует три основных правила сохранения ваших зубов в хорошей форме. Первое - чистить зубы два раза в день. Второе - посещать дантиста не реже двух раз в год. Третье - скорее не правило, а совет, в котором упоминаются не зубы, а нос. Воспроизведите третье правило.\n",
            "Answer: Не суйте нос не в свои дела.\n",
            "GPT finetuned: никак не дышать.  не дышать.  не дышать.  не д\n",
            "\n",
            "\n",
            "Question: Дуплет.  1. Какие персонажи басни Сумарокова произносят слова: \"О как, о как нам к вам, к вам, боги, не гласить\"?  2. Какие персонажи пьесы Эдмона Ростана произносят слова: \"Мы из жасминов жадно пили\"?\n",
            "Answer: 1. Лягушки.  2. Пчелы.   1. Жабы.  2. Осы, шмели.\n",
            "Comment: Эти примеры построены на звукоподражании кваканью и жужжанию.\n",
            "GPT finetuned: 1. Овцы.  2. Слоны.   1. Овцы.  2. Сло\n",
            "\n",
            "\n",
            "Question: Нарисуйте 11 клеток. Представьте, что каждая клетка - хлев. А теперь - внимание, детский вопрос! Разместите в эти 11 хлевов 10 коров, причём заполнить нужно каждый. Во избежание дуалей скажу, что нельзя становиться в пустой хлев самому или заполнять его, скажем, навозом. В вашем распоряжении только ручка, бумага и десять коров. Что же делать?\n",
            "Answer: вписать в клетки десять коров - по букве в клетку\n",
            "GPT finetuned: Нарисованные клетки - кроссворд, а коровы - 10 л\n",
            "\n",
            "\n",
            "Question: Оспу начали прививать в России с осени 1768 года. Из Англии был выписан доктор Димсдаль, и первой его пациенткой стала Екатерина II, вторым Г. Орлов; через неделю после них прививке подвергся третий. Кто был третий?\n",
            "Answer: Павел I.\n",
            "GPT finetuned: царевич Павел.  Павел Петрович, Павел Петрович 1-й\n",
            "\n",
            "\n",
            "Question: По мнению Марка Твена, афоризм - это когда максимум смысла при минимуме чего?\n",
            "Answer: Слов.\n",
            "GPT finetuned: номер символов.  символов.  слов.  букв.  знаков. \n",
            "\n",
            "\n",
            "Question: Английские ученые вычислили математическую формулу этого: P+(5xE)+(3xH). P - это личностная характеристика (мировоззрение. способность переносить неприятности и степень адаптации к новым условиям), E - бытие (состояние здоровья, финансовая стабильность, способность дружить), H - индекс высших стандартов (самоуважение, амбиции, чувство юмора). Формула чего?\n",
            "Answer: Счастья.\n",
            "GPT finetuned: щасла.  счастья.  счастливости.  счастливого л\n",
            "\n",
            "\n",
            "Question: Обувщики итальянского города Виареджо, стремясь добиться повышения зарплаты, забастовку не объявили, а, наоборот, продолжали работать дальше и даже с более высокой производительностью. Каким же образом они смогли повлиять на своих работодателей?\n",
            "Answer: Стали делать обувь только на одну ногу.\n",
            "GPT finetuned:  Они стали делать обувь в два раза быстрее,  т.к.  днём\n",
            "\n",
            "\n",
            "Question: В конце сентября прошлого года автор вопроса поехал из Лондона в Рио-де-Жанейро. При этом, разумеется, пришлось перевести часы на 4 часа назад. На сколько пришлось перевести часы по возвращении полтора месяца спустя?\n",
            "Answer: На 2 часа.\n",
            "Comment: За это время Англия перешла на зимнее время, а Бразилия - на летнее.\n",
            "GPT finetuned: 3 часа назад.  2:30, 2 часа 30 минут, 2\n",
            "\n",
            "\n",
            "Question: Бывший руководитель Польши Войцех Ярузельский часто ходил в темных очках. Причина - якобы в том, что его семью в начале 40-х сослали в Сибирь, и на Алтае Ярузельский был ослеплен ИМ и сжег роговицу. У олимпийского чемпиона Тамаша Дарньи, который как-то в детстве не успел уклониться, из-за НЕГО почти ослеп левый глаз. Назовите ЕГО.\n",
            "Answer: Снег.  Снежок.\n",
            "GPT finetuned: лёд.  Лед.  Снег.  Снеговик.  Снеговик\n",
            "\n",
            "\n",
            "Question: Завсегдатай ипподромных бегов Владимир Герцик готовил как-то для эфира сообщение о доблестном милиционере, спасшем тонувшую женщину. Включается микрофон, и в эфир летят слова: \"Тонула лошадь...\". Зловещая пауза, после которой Герцик берет себя в руки и продолжает: \"А рядом тонула женщина\" - и далее по тексту. Через неделю Радиокомитет был засыпан письмами с одним и тем же вопросом. Каким?\n",
            "Answer: А что же стало с лошадью?.\n",
            "GPT finetuned: Что это было за лошадь?  По смы\n",
            "\n",
            "\n",
            "Question: Эммануил Ласкер был чемпионом мира 27 лет и считал, что умеет определять характер и способности человека по стилю игры. Однажды в Москве к нему подошла женщина и показала пожелтевший листок бумаги с какой-то старой партией. Ласкер сказал: \"Ваш родственник излишне упрям, плохой психолог и к тому же туповат и лишен чувства юмора. Боюсь, что из него ничего не выйдет\". \"Но ведь это же ваша собственная партия!\" - рассмеялась женщина. Ласкер ухмыльнулся, потом на несколько секунд задумался и дал объяснение: \"Все правильно. Вот ...\". Закончите его фразу.\n",
            "Answer: ... Вот Капабланка меня и обыграл!\n",
            "GPT finetuned: А я\n",
            "\n",
            "\n",
            "Question: Американский психолог и писатель ТИмоти ЛИри подготовил на основании \"БардО Тхёдол\" руководство по употреблению психоделических веществ. \"БардО Тхёдол\" также называют тибетской ЕЮ. Назовите ЕЕ двумя словами.\n",
            "Answer: Книга мертвых.\n",
            "Comment: Как книга мертвых содержит подробное описание состояний, через которые проходит сознание человека начиная с процесса физического умирания, так руководство Лири описывает происходящие под действиям наркотиков изменения в человеческом сознании. Добро пожаловать в наш тур!\n",
            "GPT finetuned: 8-я книга.  Книга мертвых.  Книга мертвецов\n",
            "\n",
            "\n",
            "Question: Первую в своей истории олимпийскую победу праздновало почти всё население небольшого государства. Многочисленные ИКСЫ содержали напоминание и о сопернике, который в финале был просто порван. Назовите ИКС двумя словами, начинающимися на одну и ту же букву.\n",
            "Answer: Флаг Фиджи.\n",
            "Comment: На Олимпийских играх в Рио-де-Жанейро в 2016 году Республика Фиджи завоевала первую в своей истории олимпийскую медаль, причем сразу золотую. Сборная Фиджи по регби-7 в финале разгромила сборную Великобритании со счетом 43:7, можно сказать, \"порвала на британский флаг\". А на флаге Фиджи в левом верхнем углу как раз и красуется изображение этого флага. Фиджийцы хотели поменять флаг и даже выделили средства, но решили пока этого не делать, направив деньги на ликвидацию последствий тайфуна.\n",
            "GPT finetuned: 21-й флаг.  Флаг 21.  Флаг двадцать один\n",
            "\n",
            "\n",
            "Question: В научно-популярном фильме Би-Би-Си \"Зооолимпиада\" показано, как могли бы выглядеть соревнования в прыжках, беге и т.п. среди животных. В этой \"олимпиаде\" принимают участие команды млекопитающих, птиц, насекомых, рыб и рептилий. В русской лицензионной версии текст читают два известных спортивных комментатора. За какую команду они неприкрыто болеют?\n",
            "Answer: За птиц.\n",
            "Comment: Комментируют Виктор Гусев и Василий Уткин.\n",
            "GPT finetuned: лягушек.  за лягушек.  за лягушку.  за ля\n",
            "\n",
            "\n",
            "Question: На картине Леонардо \"Тайная вечеря\" изображен момент, когда Христос говорит: \"Один из вас предаст меня\". Один из апостолов при этом поднимает палец, и философ Лев Карсавин считал, что этот жест подчеркивает известную черту его характера, и комментировал это, вкладывая в уста апостола короткий вопрос. Мы не спрашиваем имя апостола - сформулируйте сам вопрос.\n",
            "Answer: Один ли?  Только один? и другие формулировки с тем же смыслом.\n",
            "Comment: Апостол Фома и тут усомнился и показал один палец, спрашивая: \"Один ли?\"\n",
            "GPT finetuned:  Есть ли у меня рука?.  Есть ли у меня рука?.  Есл  н\n",
            "\n",
            "\n",
            "Question: Рассказ Чехова \"Обер-верхи\". Раздел \"Верх гражданственности\": \"Я сын почтенного потомственного ПЕРВОГО, читаю \"ВТОРОЙ\", хожу в ТРЕТЬЕМ и пребываю в ЧЕТВЕРТОМ. Как вы уже догадались, вам придется назвать и ПЕРВОГО, и ВТОРОЙ, и ТРЕТЬЕ, и ЧЕТВЕРТЫЙ.\n",
            "Answer: Гражданин, Гражданин, гражданское платье, гражданский брак.\n",
            "GPT finetuned: 1. Гражданин, 2. Комедиант, 3. Театр, 4.\n",
            "\n",
            "\n",
            "Question: В компьютерной игре \"Первая мировая\" индикатор загрузки выполнен в виде НЕЕ. ОНА может быть холщовой или металлической. Назовите ЕЕ двумя словами.\n",
            "Answer: Патронная лента.  Пулеметная лента.\n",
            "GPT finetuned: лента Паттена.  Лента Паттена, лента Паттена. \n",
            "\n",
            "\n",
            "Question: Несчастье посетило Эллочку в тот момент, когда она примеряла очень миленькую крепдешиновую кофточку. В этом наряде она казалась богиней. Чувства, которые ее переполняли, можно было бы выразить так: \"Увидев меня такой, мужчины взволнуются. Они задрожат. Они пойдут за мной на край света, заикаясь от любви. Но я буду холодна. Разве они стоят меня?\". Однако Эллочка нашла более прозаичное слово \"Хо-Хо\". Что же помешало ей выразиться более развернуто?\n",
            "Answer: Ее словарный запас составлял всего 30 слов.\n",
            "GPT finetuned: Она упала и слома\n",
            "\n",
            "\n",
            "Question: Среди мотивов, по которым совершаются тяжкие преступления, этот встречается не так уж и редко, что, кстати, нашло отражение и в классической литературе. А если в определении мотива две буквы поменять местами, то такое преступление, по мнению Никиты Богословского, можно объяснить так: очень уж надоела (или надоел)! Назовите оба мотива.\n",
            "Answer: Ревность и верность.\n",
            "GPT finetuned: жадность и зависть.  \n",
            "\n",
            "\n",
            "Question: В Древнем Риме эти помещения в домах всегда сооружались с большой пышностью и тщательностью и должны были, наряду со спальней, выходить окнами на восток. В поместье Плиния Младшего, например, это помещение было сделано в виде закругленной комнаты, чтобы солнце, двигаясь по небу, всегда заглядывало в ее окна. А для чего обычно предназначались такие помещения?\n",
            "Answer: Для хранения книг.\n",
            "Comment: Речь идет о библиотеках.\n",
            "GPT finetuned:  Для хранения тел усопших.  Для похорон.  Для  мертвых.\n",
            "\n",
            "\n",
            "Question: Дуплет.  1. Отца Ричарда Дэвида Фэлко звали Тони, а приемного отца - Натаном. Под каким прозвищем из двух слов Ричард Дэвид Фэлко стал известен?  2. ЕГО внук Омар Фароян в свое время играл за молодежный состав киевского \"Динамо\" и помогал товарищам по команде возвращать украденные иномарки. Назовите ЕГО.\n",
            "Answer: 1. Сын Сэма.  2. Дед Хасан.   2. Аслан Усоян.\n",
            "Comment: 1. Ричард Дэвид Фэлко, он же Ричард Берковитц, был известным серийным убийцей, но сыном Сэма с точки зрения генеалогии не был.    2. Дед Хасан, наоборот, действительно был дедом.\n",
            "GPT finetuned: 1. Спайк Ли.  2. Армен Джигарханян.   1. Спай\n",
            "\n",
            "\n",
            "Question: Во время правления Елизаветы I Английской испанское посольство затратило немалую сумму на подкуп королевских прачек. Тот же вопрос мог заинтересовать, например, шведское или французское посольство. А в чем он состоял?\n",
            "Answer: может ли королева иметь детей.\n",
            "GPT finetuned:  Кто будет одевать королеву при родах? \n",
            "\n",
            "\n",
            "Question: Какими двумя словами не только в Британии, но и во всем мире называют Елизавету Анджелу Маргарет Боулз-Лайон?\n",
            "Answer: Королева-мать.\n",
            "GPT finetuned:  Queen Elizabeth.  Елизавета, королева Елизавета.  Ел.  Е. \n",
            "\n",
            "\n",
            "Question: Знаменитый психиатр Полю Дюбуа сравнил ЕГО с голубкой: голубка, спокойно протяни к ней руку, на нее сядет, но попробуй схватить ее - она тут же улетит прочь. А как по-латыни называется бредущий в НЕМ?\n",
            "Answer: Сомнамбула.\n",
            "GPT finetuned:  Somnambulismus.  Сомнамбулизм, сомнамбул.  Люб\n",
            "\n",
            "\n",
            "Question: В XVIII веке артиллерией русской армии управлял Артиллерийский приказ, затем - Главная артиллерийская канцелярия. А как это ведомство называлось в XVII веке?\n",
            "Answer: Пушкарский приказ.\n",
            "GPT finetuned: 1600-1700 гг. - Артиллерийский приказ, 1700\n",
            "\n",
            "\n",
            "Question: -ОСО-  1. ТАК называется расстройство координации движения глаз.  2. Назовите ОРГАНИЗАЦИЮ, правопреемником которой явилось ДОСААФ.  3. На ЭТОЙ НИЗИНЕ в 1389 г. сербы и боснийцы дрались с турками.  4. ЭТО единственное растительное МАСЛО, которое при нормальной температуре не является жидким.  5. ТАК называли участников народного восстания в Нормандии в 1639 году.\n",
            "Answer: 1. КОСОглазие.  2. ОСОАВИАХИМ.  3. КОСОво поле.  4. КокОСОвое масло.  5. БОСОногие.\n",
            "GPT finetuned: 1. Склероз.  2. ОСОАВИАХИ\n",
            "\n",
            "\n",
            "Question: Его вклад в развитие Англии мог бы быть огромен, если бы не конфликт с церковью. Неудивительно - ведь он начал выпускать первую на территории современной Великобритании газету, ему же приписывается открытие первых общедоступных школ. Начал он свой путь просветителя под деревом, а кончил - в пещере. А скажите, где он родился?\n",
            "Answer: В штате Коннектикут. Речь о Приключениях янки при дворе короля Артура М. Твена.\n",
            "GPT finetuned: 30 марта 1564 г. в Стратфорде-на-Эйвоне. Это\n",
            "\n",
            "\n",
            "Question: Во время Ялтинской конференции Уинстон Черчилль выразил желание посетить деревню Бурлюк под Балаклавой - бывшее имение семьи знаменитого поэта-футуриста Давида Бурлюка. Но к футуризму Черчилль был совершенно равнодушен, как и многие англичане, посещавшие те места до революции. А что же потянуло его туда?\n",
            "Answer: Там расположена знаменитая Долина смерти, где в годы Крымской войны погибла бригада английской легкой кавалерии, включавшая представителей самых аристократических фамилий, в том числе и Мальборо.\n",
            "GPT finetuned: 18-летний сын Бурлюка, тоже поэт-футурист, в 1\n",
            "\n",
            "\n",
            "Question: Что засвидетельствовала в 1429 году специальная дамская комиссия, возглавляемая тещей короля?\n",
            "Answer: Жанна д'Арк - девственница.\n",
            "GPT finetuned: 1429 - год рождения Жанны д'Арк.  1429 - год рож\n",
            "\n",
            "\n",
            "Question: В конце 19 века в Париже выходила специальная газета. Тираж ее был мал, что, возможно, обуславливало относительно высокую стоимость. Газета помещала сведения о торговых мероприятиях, благотворительных базарах и распродажах, а также объявления типа: \"Ищу слепого, немного играющего на флейте\". Для кого предназначалась эта газета?\n",
            "Answer: Для нищих.\n",
            "Comment: Газета предназначалась той части нищих, которая относилась к нищенству как к серьезному ремеслу.\n",
            "GPT finetuned:  Для  бродяг.  Для  бездомных.  Для  нищих.  Для  тр\n",
            "\n",
            "\n",
            "Question: По утверждению известного украинского барда Тризубого Стаса, истинный смысл одного из шлягеров Алены Апиной заключается в тоске по украинской производительнице одного из пищевых продуктов. Процитируйте строчки, которые свидетельствуют об этом.\n",
            "Answer: Ой, Льоха, Льоха, мне без тебя так плохо....\n",
            "Comment: \"Льоха\" по-украински значит \"свинья\".\n",
            "GPT finetuned:  Я так хочу, чтобы ты была рядом,  Я так хочу, чтоб твои \n",
            "\n",
            "\n",
            "Question: В 1709 году фирма, основанная Дж.Феминисом, начала выпускать раствор, названный изобретателей \"О адмирабль\". Во время Семилетней войны с этим раствором познакомились французы, которые стали называть его по-своему. Внимание, вопрос: В каком городе с 1692 года жил Дж.Феминис?\n",
            "Answer: В Кельне (О де Колон - кельнская вода, одеколон).\n",
            "GPT finetuned: 1692 - год рождения Дж.Феминиса. Он родился в 1692 \n",
            "\n",
            "\n",
            "Question: Византийцы называли его Скифским, русские - Понтским, итальянцы - Великим, арабы - Русским. Как называем его мы?\n",
            "Answer: Черное (море).\n",
            "GPT finetuned:  Черное море.\n",
            "\n",
            "\n",
            "Question: Юрий Воробьевский написал книгу о непорядочности масонских организаций. Одна из ее частей называется двумя словами, отличающимися одной буквой. Напишите эти слова.\n",
            "Answer: Ложь лож.  Ложь ложи, ложи лжи.\n",
            "GPT finetuned: 42 ложа.  Двадцать две ложи.  Двадцать две ложы\n",
            "\n",
            "\n",
            "Question: В последнее время распространилась мода на специальные чехлы на автомобильных сиденьях, состоящие из большого количества пластмассовых роликов. Сидеть на них достаточно неудобно. Зачем же они применяются?\n",
            "Answer: От радикулита.\n",
            "GPT finetuned:  Для защиты сидений от пыли и грязи. \n",
            "\n",
            "\n",
            "Question: Уважаемые знатоки! Мы уверены, что многие из вас сделали это, сознавая себя частью мирового сообщества, чтя память предков, передавших вам веру в добро и справедливость, исходя из ответственности перед нынешним и будущими поколениями. Мы не спрашиваем, что конкретно было сделано. Назовите число, месяц и год, когда это произошло.\n",
            "Answer: 12 декабря 1993 года.\n",
            "GPT finetuned: 1 января 2000 года.  1 января, 2000 год.  1 янв\n",
            "\n",
            "\n",
            "Question: По некоторым сведениям, в этой древней стране пастухи на зиму загоняли стада в укрытия и переставали их выпасать. Если вдуматься в этот факт, можно сделать вывод, что одну важную государственную дату надо перенести во многих странах - но, как ни странно, не в этой самой стране. А что это за дата?\n",
            "Answer: День НОВОГО ГОДА (ибо Рождество не могло быть зимой).\n",
            "Comment: именно Новый Год, а не Рождество. Например, в Израиле Рождество вовсе не является государственной датой.\n",
            "GPT finetuned: 1 января.  Новогодний праздник.  Новый год.  Рождённ \n",
            "\n",
            "\n",
            "Question: Дуплет.  1. Рассказывают, что Наполеон в качестве НЕГО использовал измельченный морской песок с добавкой опиума. Назовите ЕГО двумя словами.  2. Рассказывают, что белый конь Наполеона служил источником материала для НЕЕ. Назовите ЕЕ двумя словами.\n",
            "Answer: 1. Зубной порошок.  2. Зубная щетка.\n",
            "Comment: Конский волос использовался для изготовления зубных щеток императора. Кстати, считается, что белый волос - мягче. По всей видимости, опиум использовался в качестве обезболивающего.\n",
            "GPT finetuned: 1. Зубной порох.  2. Зубная паста.   1. Зу\n",
            "\n",
            "\n",
            "Question: Некая О'Хейр подала в Верховный Суд США заявление. Она возмущена тем, что с 1884 года государство давит на психику неверующих пропагандой. Где же она увидела эту пропаганду?\n",
            "Answer: на долларах\n",
            "GPT finetuned: 50 долларовых банкнотах. На них изображены 50 лж 1\n",
            "\n",
            "\n",
            "Question: Издатель Жан Ле Мэр напечатал шеститысячным тиражом книгу Декарта \"Рассуждения о методе\". В уплату Декарт получил всего 200... 200 чего?\n",
            "Answer: Экземпляров.\n",
            "Comment: Экземпляров своей книги.\n",
            "GPT finetuned: 200 экземпляров книги.  200 экземпляров, 200\n",
            "\n",
            "\n",
            "Question: Спустя два года после изобретения фотографии французским ученым Луи Дагерром был арестован его ассистент - Андре Лефевр. А за что?\n",
            "Answer: Лефевр пытался продать фотографическую карточку непристойного содержания.\n",
            "GPT finetuned: 1839 г. - ассистент Дагера Андре Лефевр был ареств\n",
            "\n",
            "\n",
            "Question: В 1970-х, после долгих десятилетий эмиграции, Россию посетил Игорь Стравинский. Один из газетчиков спросил его, какую из русских птиц он больше всего любит. В ответ композитор изобразил руками силуэт птицы, несуществующей в природе. В чем заключается ее характерная особенность, как раз и показанная Стравинским?\n",
            "Answer: У нее две головы.\n",
            "Comment: Стравинский имел в виду двуглавого орла - герб Российской империи.\n",
            "GPT finetuned: 2 головы.  Двуглавая птица.  Двухголовая птица. \n",
            "\n",
            "\n",
            "Question: Юморист Константин Егоров утверждает, что в честь котов слагают песни, пишут оперы и даже назвали одну страну и один северный российский город. Назовите эту страну и этот город.\n",
            "Answer: Кот-д'Ивуар и Мурманск.\n",
            "GPT finetuned:  Турция и Архангельск. Турция - страна котов, а  Архангельск -\n",
            "\n",
            "\n",
            "Question: Мелких летучих мышей из рода Rouzetus называют также могильными, или фараоновыми, летучими мышами. С какими их особенностями это связано?\n",
            "Answer: Они живут в могильных склепах и пирамидах.  \n",
            "GPT finetuned:  Они охотятся на мух, летающих над гробами.  \n",
            "\n",
            "\n",
            "Question: Дуплет. Два вопроса по 30 секунд обсуждения каждый.  1. Чуть ли не главная достопримечательность немецкого городка Вецлар - могила молодого секретаря брауншвейгского посольства, который в 1772 году пустил себе пулю в лоб. Назовите юриста, проходившего в том же году практику в суде города Вецлар.  2. В Брюсовом переулке в Москве сохранился старый дом, где жила французская модистка Луиза Симон-Деманш, в смерти которой обвиняли ее любовника. Напротив находилась усадьба Плещеевых, в которой в 1792 году жил... Кто?\n",
            "Answer: 1. Иоганн Вольфганг Гете.  2. Николай Михайлович Карамзин.\n",
            "Comment: 1. Молодой секретарь стал прототипом Вертера.    2. Есть сходство между судьбами Луизы и бедной Лизы, хотя \"Бедная Лиза\" была написана задолго до смерти модистки.    Оба произведения вызвали в свое время эпидемии самоубийств.\n",
            "GPT finetuned: 1. Александр Суворов.\n",
            "\n",
            "\n",
            "Question: Внимание, каверзный вопрос!  Автор вопроса - поклонник творчества питерского поэта Константина Арбенина - мечтает завести двух котят определенной породы и дать им клички, совпадающие с компонентами популярного алкогольного коктейля. Назовите эту породу.\n",
            "Answer: Сфинкс.\n",
            "Comment: В песне группы \"Зимовье Зверей\" упоминаются \"два юных сфинкса Джин и Тоник\", да и строчки \"И сфинксов он пугал ответом / На каждый каверзный вопрос\" - тоже оттуда; впрочем, вопрос берется и без знания творчества Арбенина - по ключевым словам \"питерский\", \"кошачья порода\" и \"каверзный вопрос\". :-)\n",
            "GPT finetuned:  Сфинкс.  Сфинксы.  Сфинксы.  Сфинксы\n",
            "\n",
            "\n",
            "Question: Сумо  1. Такое поэтичное название носят турниры по сумо.  2. Именно столько россиян выступает в высшем дивизионе сумо - макуути.  3. Именно в этот день в Японии закончился последний басё 2005 г.  4. Перед началом каждого турнира дохё, т.е. обмазанный глиной помост, на котором проходят бои, чтобы вызвать удачу и получить благословение богов, поливают именно этим.  5. Как шутят японцы, борцы сумо делают это не как они, а как европейцы.\n",
            "Answer: 1. Басё.  2. Двое (Сослан и Батраз Бораздовы).  3. Сегодня.  4. Сакэ.  5. Едят (ложкой).\n",
            "GPT finetuned: 1\n",
            "\n",
            "\n",
            "Question: Алексей Беляков делится впечатлениями о путешествии по Вьетнаму. Он пишет, что самое главное вьетнамское созвучие - \"синь ча!\". Это универсальное приветствие, которое можно сказать и девочке, торгующей кепками, и главе компартии. Следующий по важности возглас [ведущему: прочитать с вьетнамским акцентом :-)]: \"Уан дола!\". Это, по сути, фонетическое выражение вьетнамской экономической модели. Переведите этот возглас на русский язык.\n",
            "Answer: Один доллар.\n",
            "GPT finetuned: Деньги.  Доллара.  Долларов\n",
            "\n",
            "\n",
            "Question: БУКВА Ы  1. Именно таков порядковый номер этой буквы в алфавите?  2. Сколько букв \"Ы\" в названии столицы Тувы?  3. Так называется народный сказитель у кумыков?  4. Это - левый приток Вилюя, находящийся в Якутии?  5. Имя этого полководца государства Когуре, связано с разгромом китайцев в 612 году.\n",
            "Answer: 1. 29  2. Кызыл-2  3. Ырчи  4. Ыгыатта  5. Ыльчи Мундок\n",
            "GPT finetuned: 1. Девятая  2. Кызыл  3. ТЫЫЫЫЫЫЫЫ\n",
            "\n",
            "\n",
            "Question: На одном из сайтов автор вопроса обнаружил пародийную иллюстрацию к известному советскому фильму \"Джентльмены удачи\", на которой герои заменены персонажами мультфильмов о Винни-Пухе. Какой предмет на карикатуре изображен вместо шлема Александра Македонского?\n",
            "Answer: Горшок.  Горшок меда, Горшок для меда.\n",
            "Comment: (pic: 20120547.jpg)\n",
            "GPT finetuned: 3D-очки.  Очки для просмотра 3D-фильмов.  Очкы\n",
            "\n",
            "\n",
            "Question: Согласно \"Энциклопедии снов\", если мужчине снится, что он носит очки - это означает неприятности, а если усы - это означает смешные претензии. Причем, каждое из этих толкований верно не всегда, а при соответствующем условии. Назовите оба условия.\n",
            "Answer: Если наяву он не носит, соответственно, очков и усов.\n",
            "GPT finetuned: 1) Если мужчина снится, что он носит очки, 2) 1) \n",
            "\n",
            "\n",
            "Question: Картина Веронезе \"Свадьба в Кане\" так и не вернулась в Венецию, откуда была увезена по приказу Наполеона. Сейчас на ее старом месте висит копия. Можно сказать, что копии повезло больше, чем оригиналу: от оригинала практически все посетители сразу отворачиваются. Причину этого можно назвать двумя именами собственными. А можно и одним. Назовите в ответе любой вариант.\n",
            "Answer: Мона Лиза, Джоконда.  Любой из приведенных вариантов, а также Mona Lisa или Gioconda (La Gioconda).\n",
            "Comment: В Лувре картина висит прямо напротив \"Джоконды\" (да-да, \"Джоконду\" недавно перенесли из Большой Галереи в зал).\n",
            "GPT finetuned:  Мона Лиза.  Джоконда.  Даная.  Венера.  Афродит\n",
            "\n",
            "\n",
            "Question: В христианстве одной из причин почтительного отношения к павлинам считается то, что хвост павлина [ПРОПУСК]. В спектакле Вадима Сквирского Раскольников сидит за читающей монолог Сонечкой, а миска в его руках [ПРОПУСК]. Какие два слова, начинающиеся на одну и ту же букву, мы дважды пропустили в тексте этого вопроса?\n",
            "Answer: Напоминает нимб.\n",
            "Comment: Так режиссер усиливает \"святость\" того, что Мармеладова говорит Раскольникову.\n",
            "GPT finetuned:  Напоминает рог.  Напоминает рог.  Напоминает рёг. \n",
            "\n",
            "\n",
            "Question: Хотя он никогда не был членом русской дружины (а ведь мог бы!), его эпитеты - Великий и, что менее соответствует его имени, Грозный. Благодаря превосходству над своими могучими противниками он дошел до четырехсотой, пообещав хранить ее на память. О ком идет речь?\n",
            "Answer: Об Александре Могильном.\n",
            "Comment: Могильный принципиально не играет за сборную России по хоккею. Александр Великий в истории был, а Александра Грозного не припоминается. В матче этого сезона с \"Аннахайм Майти Дакс\" (\"Аннахаймские Могучие Утки\") он забросил свою 400-ю шайбу в НХЛ. Хоккейные команды часто называют ледовыми дружинами. \"Вторая шайба Могильного, сделавшая счет 4:0, стала его четырехсотой в НХЛ. \"Сначала я не знал, что это четырехсотый гол, - жаловался во втором перерыве Александр Великий, - иначе я бы обязательно сразу сохранил бы шайбу. Мне сказали о юбилее, когда я был уже на скамейке, а игра продолжалась. Надеюсь, никто не возьмет себе шайбу раньше меня\". В конце концов, Могильный отжал-таки свою юбилейную шайбу и заявил, что будет держать ее в шкафу.\n",
            "GPT finetuned: 1001 ночь. Речь идет о Шахрезаде.\n",
            "\n",
            "\n",
            "Question: Обследовав в Африке более 400 слонов, врачи обнаружили, что они страдают склерозом. Это выбило почву из-под ног проповедников вегетарианства. В чем тут дело?\n",
            "Answer: Считается, что у травоядных склероза быть не должно.\n",
            "GPT finetuned: 90% слонов страдают склерозом из-за того,что 90% \n",
            "\n",
            "\n",
            "Question: Перед комбинатом, где изготовляют эти кружки, неподалеку от малайзийской столицы Куала-Лумпура, высится кружка, увеличенная в 25 раз, что и было зафиксировано в \"книге рекордов Гиннесса\". Для чего предназначались эти специально изготовляемые кружки?\n",
            "Answer: Для пива Гиннесс.\n",
            "GPT finetuned:  Для проверки оптических приборов.  Для проверк  оптич. приборов\n",
            "\n",
            "\n",
            "Question: Рецептов быть и оставаться счастливым много. Удивительно, но один и тот же рецепт предложили поэт Гораций и герой Николая Носова Пачкуля Пестренький. Что это за рецепт?\n",
            "Answer: Ничему не удивляться.  4/8\n",
            "GPT finetuned:  Счастье - это то, что не начинается и не кончаетсл.  Пр\n",
            "\n",
            "\n",
            "Question: Дуплет.  1. Саша Гитри однажды заметил, что \"читать полезно - глаза не так устают\". Разгадав смысл этого афоризма, ответьте, какие два слова мы в нем пропустили.  2. Саша Гитри однажды заметил, что \"если о женщине хотят сказать что-то хорошее, используют ЕГО, если плохое - ИХ\". Что мы заменили на \"ЕГО\" и \"ИХ\"?\n",
            "Answer: 1. ... между строк....  2. Единственное число, множественное число.\n",
            "GPT finetuned: 1. Вместо.  2. Вместо, вместо.   1. Вместо.  2\n",
            "\n",
            "\n",
            "Question: Чарли Чаплин в своей биографии рассказывает о комедии \"На плечо!\", снятой в 1918 году, так: \"Начало должно было изображать жизнь дома, середина - \"войну\", а конец - \"банкет\", на котором все монархи Европы чествовали бы меня за героическое пленение кайзера, а под занавес я, разумеется...\" Так что же должно было произойти с ним в конце комедии?\n",
            "Answer: Он просыпался.\n",
            "GPT finetuned:  Я должен был умереть.  Он должен был умереть.  Умр. \n",
            "\n",
            "\n",
            "Question: Этот превосходящий титул встречается первый раз в уставе Петра Великого, а вспомните двух человек, которые кроме принцев царской крови, его носили до XX века.\n",
            "Answer: Суворов и Меншиков (генералиссимус).\n",
            "GPT finetuned: царевич Алексей и царевич Алексей. (Царевич - л. 1,\n",
            "\n",
            "\n",
            "Question: На римском форуме Антонин Пий в 141 году построил храм в память своей молодой жены Фаустины Старшей. Сейчас на фасаде храма можно прочесть: \"Божественному Антонину и божественной Фаустине по постановлению сената\". Каких слов не было первоначально?\n",
            "Answer: Когда умерла его жена, он приказал написать: Божественной Фаустине по постановлению сената, а после его смерти сенат добавил: Божественному Антонину и....\n",
            "GPT finetuned: царевичу и царевне. Фаустина была дочерью царевнёй\n",
            "\n",
            "\n",
            "Question: Согласно преданиям, задолго до появления на Урале башкир и русских здесь жили старые люди, которые не интересовались богатством и довольствовались тем, что могли добыть собирательством и охотой. Для охоты же они использовали и то, что после появления здесь других народов, по совету подобранного ими раненого казака, собрали и скрыли в тайной пещере. А то, что не собрали, не нашли - и сейчас изредка попадается людям. Так предание объясняет нахождение на Урале в самых неожиданных местах... Чего?\n",
            "Answer: Золотых самородков.\n",
            "Comment: Камешки увесистые, для охоты удобные - в зверя или птицу кидать. Старые люди их порастеряли, а золотоискатели находили - там, где золота, вроде, и быть не должно.\n",
            "GPT finetuned: Золото.  Золотые монеты.\n",
            "\n",
            "\n",
            "Question: Статья \"Как хранить вино\" справочника-путеводителя \"Вина Испании\" повествует о сложностях создания винного погреба в условиях современной квартиры. Воспроизведите состоящий из трех слов эпиграф к этой статье.\n",
            "Answer: И немедленно выпил.\n",
            "Comment: Поскольку хранить вино в условиях современной квартиры крайне затруднительно, автор статьи рекомендует его не хранить.\n",
            "GPT finetuned: 1001 ночь.  Тысяча и одна ночь.  1001 ночь,\n",
            "\n",
            "\n",
            "Question: В классическом искусстве дизайна помимо ваз для цветов используются изящные тюльпанницы, использующиеся с периода Нового Времени, когда тюльпан считался одним из самых дорогих и изящных цветов. В чем же их различие?\n",
            "Answer: У тюльпанницы - отдельное горло для каждого цветка.\n",
            "GPT finetuned:  Тюльпанницы - это вазы для тюльпанов, а тюлпан  - \n",
            "\n",
            "\n",
            "Question: В.Белинский говорил, что отделить нравственность от искусства так же невозможно, как разложить огонь на силу горения и... Назовите остальные две составляющих.\n",
            "Answer: Свет и теплота.\n",
            "GPT finetuned: 1 - теплоту сгорания, 2 - свечение.  1 - теплотн. \n",
            "\n",
            "\n",
            "Question: В 1834 году лондонская \"Глоб\" писала: \"Шагом назад в истории цивилизации является позволение, чтобы этот ужасный звук, в сравнении с которым бранный клич краснокожих может считаться образцом благозвучия, раздавался в больших городах среди тихой духовной деятельности людей и в поместьях изнеженной аристократии...\"А где применялось устройство, издававшее этот \"ужасный звук\"?\n",
            "Answer: В паровозах (паровой свисток - для предупреждения людей).\n",
            "GPT finetuned: 1834 - год введения паровой машины на пароходск 1834 -\n",
            "\n",
            "\n",
            "Question: [Ведущему: сначала прочитать общую часть в том виде, в котором она написана; номера подвопросов читать так, как они указаны.]  В результате долгого процесса из этого вопроса получился дуплет. Два вопроса, по 30 секунд обсуждения на каждый.  1. Вопрос номер РАЗ. Часть ответа на вопрос вы уже услышали. Напишите этот ответ.  2. Вопрос номер ДВА. Часть ответа на вопрос вы уже услышали. Напишите этот ответ.\n",
            "Answer: 1. Ответ номер РАЗ: ЛаврАзия.  2. Ответ номер ДВА: ГондвАна.\n",
            "Comment: По современным научным представлениям, некогда единый суперконтинент Пангея в результате долгих геологических процессов распался на два. Слова \"РАЗ\" и \"ДВА\" являются частями слов \"Лавразия\" и \"Гондвана\". Кстати, несколько менее известный единый суперконтинент РодИния содержит в себе слово \"ОДИН\".\n",
            "GPT finetuned: 1. 30.  2.\n",
            "\n",
            "\n",
            "Question: \"Мой брат-король упек меня в тюрьму,  Хожу я в маске - и в мороз, и в зной...  [...  ...]\".\n",
            "Answer: Все кажется - вот маску я сниму,  И этот мир изменится со мной\n",
            "GPT finetuned:  Всё это забыто, всё это забыто,  Всё это лжец \n",
            "\n",
            "\n",
            "Question: В восточной части Индийского океана живет рыба, дышащая воздухом. А что с ней происходит, если ей не дать заполнить воздухом свой резервуар?\n",
            "Answer: Она тонет.\n",
            "GPT finetuned:  Она умирает.  Она утонула.  Она погибает.  Она\n",
            "\n",
            "\n",
            "Question: Гуманист 15 века Лоренцо делла Вилла называл пять важнейших условий, необходимых для труда ученого: душевный покой, удобное место, свободное время, общение с образованными людьми и изобилие... Чего?\n",
            "Answer: Книг\n",
            "GPT finetuned:  книг.  рукописей.\n",
            "\n",
            "\n",
            "Question: Память о государственных лидерах обычно сохраняется в многочисленных портретах и памятниках, книгах и объектах их имени. А вот особенности восточной культуры позволили председателю Мао оставить свой след еще одним способом, о чем свидетельствуют и железнодорожный вокзал в Пекине, и самолеты авиакомпании \"Эйр Чайна\", и газета \"Жэньминь жибао\". В чем состоит этот способ?\n",
            "Answer: В их названиях использована каллиграфия Мао.\n",
            "GPT finetuned: 4-е иероглифы фамилии Мао Цзэдуна образуют 5-е\n",
            "\n",
            "\n",
            "Question: Древние финикийцы плавали по всему Средиземноморью. Какой архипелаг, по одной из версий, обязан своим названием их верховному божеству - Ваалу?\n",
            "Answer: Балеарские острова.\n",
            "GPT finetuned:  Балеарские острова.  Мальорка.  Майорка.  Менорк. \n",
            "\n",
            "\n",
            "Question: В названиях самца ПЕРВОЙ птицы и самки ВТОРОЙ птицы одинаковое количество букв. Из них только одна встречается в обоих этих словах. В названиях детенышей этих птиц также одинаковое количество букв. Но более того, эти слова различаются только своей второй буквой. О каких птицах идет речь?\n",
            "Answer: Петух и цапля.\n",
            "Comment: Цыпленок и цапленок.\n",
            "GPT finetuned: жаворонок и жаворонок. \n",
            "\n",
            "\n",
            "Question: Восьмой, девятый, десятый этажи этого строения занимает уникальное устройство, которое могли бы отремонтировать фирмы Павла Буре и Рагинского, но не стали. Устройство имеет выход на четыре стороны и содержит в себе более 26 кг золота. Как называется здание, в котором размещено это уникальное устройство?\n",
            "Answer: Спасская башня московского Кремля.\n",
            "GPT finetuned:  Эйфелева башня.  Башня Эйфеля.  Эйфель. \n",
            "\n",
            "\n",
            "Question: Четыре года назад существовало 43 автопортрета Ван-Гога, а сейчас их только 37. Почему?\n",
            "Answer: 6 оказались поддельными.\n",
            "GPT finetuned: 6 погибли во время Второй мировой войны.\n",
            "\n",
            "\n",
            "Question: Голландцы непременно каждое утро за завтраком едят его. Французы лакомятся в конце обеда вместе с десертом. А в Швейцарии у некоторых есть это имеющее возраст, равный возрасту владельца. Что это такое?\n",
            "Answer: Сыр.\n",
            "GPT finetuned: 100-летний сыр.\n",
            "\n",
            "\n",
            "Question: В индейском узелковом письме кипу различные сообщения передавались с помощью окрашенных узлов. Черные узелки означали смерть, несчастье, белые - мир, красные - войну, зеленые - маис. А что означали желтые узелки?\n",
            "Answer: Золото.\n",
            "GPT finetuned:  Золото.\n",
            "\n",
            "\n",
            "Question: Ян Флеминг в одном из своих романов приводит изречение Блаженного Августина, начинающееся словами: \"Боже, сделай меня целомудренным!\". Закончите его при условии, что это изречение вполне соответствует описываемой ситуации.\n",
            "Answer: Но не сейчас!.\n",
            "GPT finetuned:  - Но не сегодня!.  - Но не сейчас!.  - Но не нынч!. \n",
            "\n",
            "\n",
            "Question: Из отчета о матче Россия - Исландия в газете \"Спорт-Экспресс\": \"Россияне безуспешно пытались найти лазейки в железобетонной обороне. Наставник исландцев выставил четырех защитников, игравших на английский манер без либеро. Впрочем, довольно часто в эту линию оттягивался еще и опорный полузащитник - тогда на подступах к штрафной соперника становилось так тесно, как...\". Закончите цитату, причем абсолютно точно.\n",
            "Answer: ... в бочке с исландской селедкой.\n",
            "GPT finetuned: ... в туалете.  ... в сортире.\n",
            "\n",
            "\n",
            "Question: Дуплет.  1. ОНА стоила дороже предшественницы, и была основана на модификации под названием \"Familiare\" [Фамильяре]. Назовите ЕЕ одним словом.  2. Описывая произведение пятидесятых годов, Наталья Войченко поясняет, что такой герой не мог быть в пионерском галстуке. Назовите это произведение.\n",
            "Answer: 1. Двойка.  2. Опять двойка.\n",
            "Comment: \"Двойка\", то есть ВАЗ-2102, стоила чуть дороже \"копейки\". Фамильяре, то есть \"семейный\", - итальянское название кузова \"универсал\", а также модификация Фиата-124, имевшая такой кузов.\n",
            "GPT finetuned: 1. Фамилия.  2. Фамилия.   1. Фамилия.  2. Фами\n",
            "\n",
            "\n",
            "Question: Слово \"гаплология\" означает некоторый процесс, которому оно само не подверглось. В чем сущность этого процесса.\n",
            "Answer: В упрощении слова путем слияния в один двух рядом стоящих одинаковых или близких по звучанию слогов.\n",
            "GPT finetuned:  Гаплология - наука об изменении числа хромосом  в  организме.\n",
            "\n",
            "\n",
            "Question: На этот вопрос будут засчитываться ответы с достаточной степенью конкретизации.  В 1980 году на экраны вышел американский художественный фильм \"Человек-слон\" о человеке с врожденным дефектом головы. Что именно появилось благодаря этому фильму в 1981 году?\n",
            "Answer: Премия Оскар за лучший грим.  По наличию слов Оскар и грим/гример (Make-Up).  Просто Оскар.\n",
            "Comment: Вроде бы выдающаяся работа гримера, а \"Оскара\" на нее не нашлось - вот и решили учредить. Сам фильм, кстати, премию не получил, ибо в 1980 году ее еще не было.\n",
            "GPT finetuned: 51-я церемония вручения премии Оскар.  51-й Оскар\n",
            "\n",
            "\n",
            "Question: В своих завоевательных походах монголы использовали типичную тактику кочевников - наваливались большим числом на слабейшего противника, и быстро рассыпались по степи, если противник был силен. Какая русская пословица произошла от описания этой тактики?\n",
            "Answer: Молодец против овец, а против молодца сам овца.\n",
            "GPT finetuned: 10000 против одного.  10000 против 1.  10000 \n",
            "\n",
            "\n",
            "Question: Нас ждут великие открытия,  Двум шлюпам путь на юг указан.  Фаддеич где?! Час до отплытия -  [...].\n",
            "Answer: Он на Востоке быть обязан\n",
            "GPT finetuned: 10 минут на Землю, 10 минут на небо!  10 мин. \n",
            "\n",
            "\n",
            "Question: Внимание, цитата: \"Облетев [три слова пропущено], я увидел, как прекрасна наша планета. Люди, будем хранить и приумножать эту красоту, а не разрушать ее!\". Кому принадлежит эта цитата?\n",
            "Answer: Ю.А. Гагарин.\n",
            "GPT finetuned: 14-летнему космонавту Юрию Гагарину.  14-летне\n",
            "\n",
            "\n",
            "Question: На монете номиналом в два евро, выпущенной в 2014 году, изображены отпечатки ботинок на песке. Какой день упомянут в надписи на этой монете?\n",
            "Answer: D-Day.  D, Д, День Д, 6 июня 1944 года.\n",
            "Comment: В 2014 году высадке в Нормандии исполнилось 70 лет.\n",
            "GPT finetuned: 28 июня.  28.06, 28.06.  28 июня 19\n",
            "\n",
            "\n",
            "Question: По мнению Марианны Хлопониной, ЭТО держится три дня, знойно пахнет корицей, дешевым ситцем, базарной пылью. А еще ЭТО приносит совершенство. Назовите с точностью до улицы адрес, куда ЭТО приносит в известном источнике.\n",
            "Answer: Лондон, Вишневая улица.  По смыслу (с учетом разных переводов).\n",
            "Comment: Имеется в виду восточный ветер, на котором прилетела Мэри Поппинс.\n",
            "GPT finetuned: 123 Болотная улица.  123-я Болотная ул., 1\n",
            "\n",
            "\n",
            "Question: \"Моим девизом всегда было \"Гляди вверх!\", и теперь мне только это и остается\". Так звучит ОНА, которую заранее подготовил астроном Джек Хоркхаймер. Назовите ЕЕ.\n",
            "Answer: Эпитафия.  Автоэпитафия.\n",
            "GPT finetuned:  Эпитафия Джеку Хоркхаймеру.  По слову эпитафия\n",
            "\n",
            "\n",
            "Question: Поэт Евгений Долматовский рассказывал о своем друге, Дмитрии Баскакове, у которого был брат. Брат этот в 30-е годы изучал, как тогда казалось Долматовскому, никому не нужный предмет. Хотя впоследствии, во время службы на Дальнем Востоке, этот предмет ему пригодился. Что это за предмет?\n",
            "Answer: Японский язык.\n",
            "GPT finetuned:  Китайский язык.  Китайская грамматика.  Китаевед.  К.\n",
            "\n",
            "Precision=0.04448\n",
            "Recall=0.05770\n",
            "F2=0.04988\n"
          ]
        }
      ],
      "source": [
        "f2_list = []\n",
        "pr_list = []\n",
        "rec_list = []\n",
        "print()\n",
        "\n",
        "def postprocess(text):\n",
        "    output = text.split(' Тур: ')[0]\n",
        "    output = re.split('\\d{1,3}/\\d{1,3}', output)[0]\n",
        "    return output\n",
        "\n",
        "model.config.use_cache = True\n",
        "\n",
        "for quest in tqdm(raw_data['test']):\n",
        "    # model.load_adapter(model_id=DEFAULT_LORA_0, adapter_name='default')\n",
        "    # response = generate(quest, model, tokenizer, repetition_penalty=1.4)\n",
        "    # print(\"GPT vicuna:\", response, '\\n')\n",
        "\n",
        "    # model.load_adapter(model_id=SAVED_LORA, adapter_name='default')\n",
        "    response = postprocess(generate_beams(quest, model, tokenizer, max_new_tokens=MAX_ANSWER_LENGTH, cutoff_len=CUTOFF_LEN_TEST))\n",
        "\n",
        "    precision, recall, f2 = calc_metrics([response, ], [quest[\"Answer\"], ])\n",
        "\n",
        "    f2_list.append(f2)\n",
        "    pr_list.append(precision)\n",
        "    rec_list.append(recall)\n",
        "\n",
        "    if recall > 0:\n",
        "        print()\n",
        "        print_dict(quest)\n",
        "        print(\"GPT finetuned:\", response)\n",
        "        print()\n",
        "\n",
        "precision = np.array(pr_list).mean()\n",
        "recall = np.array(rec_list).mean()\n",
        "f2 = np.array(f2_list).mean()\n",
        "print(f\"Precision={precision:.5f}\\nRecall={recall:.5f}\\nF2={f2:.5f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 quest = raw_data[<span style=\"color: #808000; text-decoration-color: #808000\">'test'</span>][<span style=\"color: #0000ff; text-decoration-color: #0000ff\">521</span>]                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>response = generate_beams(quest, model, tokenizer, max_new_tokens=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">20</span>, cutoff_len=CUTOFF_     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4 </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>()                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyError: </span><span style=\"color: #008000; text-decoration-color: #008000\">'test'</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
              "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1 quest = raw_data[\u001b[33m'\u001b[0m\u001b[33mtest\u001b[0m\u001b[33m'\u001b[0m][\u001b[94m521\u001b[0m]                                                                \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0mresponse = generate_beams(quest, model, tokenizer, max_new_tokens=\u001b[94m20\u001b[0m, cutoff_len=CUTOFF_     \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m3 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m4 \u001b[0m\u001b[96mprint\u001b[0m()                                                                                      \u001b[31m│\u001b[0m\n",
              "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
              "\u001b[1;91mKeyError: \u001b[0m\u001b[32m'test'\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "quest = raw_data['test'][521]\n",
        "response = generate_beams(quest, model, tokenizer, max_new_tokens=20, cutoff_len=CUTOFF_LEN_TEST)\n",
        "\n",
        "print()\n",
        "print_dict(quest)\n",
        "print(\"GPT finetuned:\", response, '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.04473945477158799\n",
            "0.053768498611859804\n",
            "0.04776260601520403\n"
          ]
        }
      ],
      "source": [
        "print(precision)\n",
        "print(recall)\n",
        "print(f2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "Question: В статье из газеты \"Секретные материалы 20 века\", посвященной адмиралу Макарову, рассказывается, в частности, о том, что еще в 1892 г. Макаров выдвинул идею снабдить головки снарядов бронебойными наконечниками. Реализация идеи в металле потребовала много времени и усилий для разрешения технических трудностей, но Макаров не отступился и добился успеха. Однако в серию колпачки не пошли. Уже из Порт-Артура, готовясь к генеральному сражению с японским флотом, адмирал затребовал для тихоокеанской эскадры два вагона колпачков, но не получил. Автор статьи замечает, что одной из причин поражения русского флота стало то, что Макарову не удалось пробить броню... Какой машины - ответьте одним словом.\n",
            "Answer: Бюрократической.\n",
            "\n",
            "1\n",
            "Question: Никулин вспоминал: \"Я не понимал, почему вокруг его имени такой бум. А спустя три года я был восхищен\". Сам он рассказывал: \"В чем только не обвиняют меня, в чем только не подозревают! Я выхожу, чтобы говорить. Я не пищу, я не кричу, я молчу\". Назовите этого парадоксального человека.\n",
            "Answer: Леонид Енгибаров.\n",
            "\n",
            "2\n",
            "Question: Согласно журналу \"Космополитен\", широкой популярностью пользуется новое политкорректное издание Библии для феминисток. В этом издании одна из ключевых формулировок приросла названием советского романа прошлого века. Укажите в ответе получившуюся формулировку.\n",
            "Answer: Бог-Отец/Мать.  С поправкой на пунктуацию.\n",
            "Comment: Добавили к формулировке \"Бог-Отец\" слово \"мать\" (одноименный роман написал Максим Горький в начале прошлого века).\n",
            "\n",
            "3\n",
            "Question: Геннадий Хазанов после гастролей в одном российском городе признался, что фраза Александра Сергеевича все еще актуальна. Назовите этот город.\n",
            "Answer: Саратов.\n",
            "Comment: \"Когда в этот раз я проехал по городу и увидел, насколько современна сегодня фраза из бессмертной комедии \"Горе от ума\": \"В деревню, к тетке, в глушь, в Саратов\", - у меня похолодела спина от того, в каком состоянии сегодня находится город\".\n",
            "\n",
            "4\n",
            "Question: На упаковке продукции компании \"Снежная панда\" есть рисунок, на котором панда выглядывает из бамбукового леса. Ответьте словом, которое пишется через дефис: какую практическую функцию выполняет этот рисунок?\n",
            "Answer: Штрих-код.\n",
            "Comment: Толстые и тонкие полоски штрих-кода стилизованы под стебли бамбука.\n",
            "\n",
            "5\n",
            "Question: Послушайте список стран: Бахрейн, Катар, ОАЭ, Иран, Ирак, Алжир, Оман, Венесуэла, Мексика... Если вы вспомните то, что приносит большую часть доходов этих стран, то вы без труда вспомните хотя бы одну из двух неназванных стран.\n",
            "Answer: Саудовская Аравия, Кувейт. Эти страны - члены ОПЕК (организации стран - экспортеров нефти)\n",
            "\n",
            "6\n",
            "Question: В этимологически точном переводе на русский язык это звучит неэстетично: Козлиный хоровод. На самом деле оно ласкает слух ценителей уже более 2-х столетий. Кто же его создатель?\n",
            "Answer: Л. Бетховен (речь идет о Рондо каприччио)\n",
            "\n",
            "7\n",
            "Question: Эта страна всегда была на периферии и потому автору сложно подобрать к ней какие-то данные иностранцами устойчивые ассоциации. Но в последней части XX века несколько представителей этой страны за свои достижения получали от журналистов прозвище, прилагательное из которого до того было устойчиво связано с представителями другой европейской страны. Вполне вероятно, что еще один представитель этой страны в ближайшее время оправдает данное ему авансом это прозвище - по крайней мере, в этом году он получил хороший старт. Впрочем, назвать мы вас попросим не его, а того, кто последним был его безусловно достоин - и притом на том же самом месте.\n",
            "Answer: Мика Хаккинен.\n",
            "Comment: Это прозвище \"Летучий финн\". Сейчас так начинают называть и Кими Райкконена, но последний на данный момент великий финский автогонщик - Хаккинен, место которого в команде \"McLaren\" занял Райкконен после ухода Хаккинена из гонок.\n",
            "\n",
            "8\n",
            "Question: С 1946-го по 1953-й год Борис написал около трех десятков стихотворений. Каждое из них было достойно антологии лучших стихов, но он \"подарил\" большую часть из них Юрию. Назовите фамилии Бориса и Юрия.\n",
            "Answer: Пастернак, Живаго.  \n",
            "Comment: Пастернак подарил стихи своему главному герою.\n",
            "\n",
            "9\n",
            "Question: Свою далеко не примитивную фамилию ОН часто не дописывал до конца. Ярослав Смеляков в одном из произведений писал, что выдал бы ему заем. Но более на слуху произведение другого отечественного поэта, посвященное ЕМУ. Назовите ЕГО.\n",
            "Answer: Нико Пиросмани.  Пиросманашвили, Пиросманишвили.\n",
            "Comment: Сам он был примитивистом, и даже свою довольно длинную фамилию часто не дописывал. Смеляков вызывался одолжить денег бедному Пиросмани, потратившему всё на актрису Маргариту. Гораздо известнее посвященные этому строки Вознесенского, послужившие основой для песни \"Миллион алых роз\".\n",
            "\n",
            "10\n",
            "Question: 21 ноября 2002 года Майкл Джексон на церемонии вручения германской премии Bambi получил награду как \"Артист тысячелетия\". В 2004 году лучшим также стал Джексон - благодаря и высоким, и не очень, но в первую очередь, конечно, благодаря профессору. Назовите этого профессора.\n",
            "Answer: Джон Рональд Руэл Толкиен.\n",
            "Comment: Питер Джексон, режиссер кинотрилогии \"Властелин колец\", получил в 2004 году \"Оскара\" как лучший режиссер (а фильм получил еще десяток статуэток).\n",
            "\n",
            "11\n",
            "Question: Тот, кто первым с ЭТИМ столкнулся, сказал, что оно похоже на помойку, а привели его туда двое усатых. Сейчас ЭТО тоже ассоциируется у нас с двумя усатыми, одного из которых, к сожалению, уже нет в живых. Назовите ЭТО.\n",
            "Answer: Поле чудес.\n",
            "Comment: Туда привели Буратино лиса Алиса и кот Базилио; передачу \"Поле чудес\" сделали популярной Владислав Листьев и Леонид Якубович.\n",
            "\n",
            "12\n",
            "Question: ТЯЖЕЛАЯ ТЕМА  1. Так расшифровывается аббревиатура КВ в названии советского тяжелого танка  2. Так называется вода, в молекулу которой вместо протия входит дейтерий  3. Название этого химического элемента происходит от греческого слова \"тяжелый\"  4. Борис Громов сменил на посту губернатора Московской области именно его  5. Этот боксер снялся в главной роли в художественном фильме \"Тяжелые перчатки\"\n",
            "Answer: 1. Клим Ворошилов  2. Тяжелая вода  3. Барий  4. Анатолий Тяжлов  5. Ласло Папп\n",
            "\n",
            "13\n",
            "Question: В эпоху ампира появились эти атрибуты великосветских дам со встроенными лорнетами и моноклями. Назовите их.\n",
            "Answer: Вееры.\n",
            "\n",
            "14\n",
            "Question: Поэтесса Елена Орлова пишет ЭТО в три слова и рифмует с телефоном. Борис Акунин пишет ЭТО через два дефиса. А орфографический словарь утверждает, что писать это НАДО, как Тургенев, в одно слово. Напишите это КАК НАДО.\n",
            "Answer: Комильфо.\n",
            "Comment: В переводе с французского: \"комильфо\" - как надо.\n",
            "\n",
            "15\n",
            "Question: Оказавшись в комнате, заполненной сплошными рюшами - на занавесках, чехлах для мебели, этажерках и даже на платье хозяйки, - персонаж повести Клод Изнер не может избавиться от ощущения, что у него вот-вот случится приступ ЕЕ. Назовите ЕЕ двумя словами.\n",
            "Answer: Морская болезнь.\n",
            "Comment: \"Волны рюшей и водовороты ткани\".\n",
            "\n",
            "16\n",
            "Question: [Ведущему: неявно выделить голосом слово \"определенный\".]  Тренер \"Арсенала\" Герберт Чэпмен хотел, чтобы клуб был всюду на первом месте. Поэтому в определенный момент отдал распоряжение навсегда убрать ЭТО. Назовите ЭТО.\n",
            "Answer: Определенный артикль The в названии клуба.  Определенный артикль, Артикль The, The.\n",
            "Comment: Для того чтобы \"The Arsenal\" находился на первом месте не только по результатам, но и по алфавиту, тренер велел убрать из названия определенный артикль.\n",
            "\n",
            "17\n",
            "Question: В одном рассказе преступники взламывают сельские магазины, где быстро находят спрятанные в укромных местах деньги. Ведь на дело они ходят не одни, а перед этим покупают в магазинах товар, помечая купюру... Чем?\n",
            "Answer: Валерьянкой.  Валерианой.\n",
            "Comment: Преступник давал продавцу крупную купюру, чтобы тот не мог использовать ее как сдачу. Далее в дело вступал кот.\n",
            "\n",
            "18\n",
            "Question: На какой московской площади можно увидеть бюсты Константина Тона, Федора Шехтеля и Алексея Щусева?\n",
            "Answer: Комсомольская.  Трех Вокзалов.\n",
            "Comment: У трех вокзалов поставлены памятники их архитекторам.\n",
            "\n",
            "19\n",
            "Question: В середине 60-х годов прошлого века на выборах в этой столице левая оппозиция добилась успеха под экологическим лозунгом \"Видеть сто лиц (ПРОПУСК) не менее ста дней в году!\". Мы не спрашиваем, какое слово пропущено. Назовите столицу.\n",
            "Answer: Токио.\n",
            "Comment: Серия гравюр Хокусая называется \"Сто лиц Фудзи\" (известна также под названием \"Сто видов горы Фудзи\"). Победившим левым удалось-таки в итоге снизить загрязненность воздуха в Токио и рассеять смог.\n",
            "\n",
            "20\n",
            "Question: Известный естествовед Плиний в опровержение бытующему тогда мнению утверждал, что вымачивая в крови козла, его можно разбить, а Альбертус Магнус уточнял, что для этого козла перед тем, как выпустить из него кровь, следует кормить петрушкой и поить вином. Кто он?\n",
            "Answer: Алмаз.\n",
            "\n",
            "21\n",
            "Question: В жизни бывают иногда странные совпадения. На Хайгейтском кладбище Лондона по соседству расположены могилы двух философов, которые при жизни были непримиримыми антагонистами. К тому же это соседство невольно заставляет многих посетителей кладбища вспомнить известную сеть. Напишите фамилии этих философов.\n",
            "Answer: Маркс и Спенсер.\n",
            "Comment: Могилы Карла Маркса и Герберта Спенсера находятся рядом, что невольно напоминает о знаменитой сети универмагов \"Маркс энд Спенсер\".\n",
            "\n",
            "22\n",
            "Question: Некий настоятель французского монастыря, заметив, что свиньи, евшие ЭТО, толстеют, решил испробовать ЭТО на монахах. Монахи умерли, а ЭТО получило название \"против монахов\". Однако алхимики еще долго спорили касательно ядовитости ЭТОГО. Именно так Михельсон в своей книге \"Русская мысль и речь. Опыт русской фразеологии\" рассказывает о возникновении этого афоризма. Назовите этот афоризм.\n",
            "Answer: Разводить антимонии.\n",
            "Comment: Речь идет о сурьме. \"Против монахов\" по-французски - \"antimoine\".\n",
            "\n",
            "23\n",
            "Question: Журналист Йоси Аргаман рассказывает в своей статье, что отказ израильской армии финансировать ему полет из Парижа в Гибралтар около сорока лет назад спас его жизнь. С тех пор его преследуют приступы клаустрофобии, которые начались, как ни странно, во время отдыха в Эйлате много лет спустя. Ответьте абсолютно точно, какому событию посвящена описанная статья.\n",
            "Answer: Гибель Даккар.\n",
            "\n",
            "24\n",
            "Question: [Разминка]  Цитата из перевода книги Норберта Винера, в которой речь идет об атомной бомбе: \"... наш возможный враг не сумеет завладеть этим секретом по крайней мере еще несколько лет, в течение которых мы сможем создать новые, еще более разрушительные...\". Далее в тексте идут два слова на русском языке, не согласующиеся с остальным предложением. Назовите их.\n",
            "Answer: Знаю как.  Любая форма глагола знать.\n",
            "Comment: Очевидно, в оригинале было \"know-how\", которые перевели дословно.    Вопрос посвящается нежно любимой нами команде \"Know How\".\n",
            "\n",
            "25\n",
            "Question: [Ведущему: отточия в цитате не выделять!]  А.М. Горький. \"Дело Артамоновых\". Цитата:  \"- Ишь ты, серопузый...  - Господа! Нашему сословию есть на что опереться...! Нам не надо мудрецов, мы сами - с усами; нам одно надобно: чиновники другие!\"  Назовите мудреца, о котором шла речь в предыдущем абзаце.\n",
            "Answer: Архимед.\n",
            "Comment: \"Сын мой, Мирон, умник, будущий инженер, сказывал: в городе Сиракузе знаменитейший ученый был; предлагал он царю: дай мне на что опереться, я тебе всю землю переверну!\" А \"серопузый\" - по созвучию с Сиракузами.\n",
            "\n",
            "26\n",
            "Question: С этих двух слов начинается предупреждение, которое, впрочем, мало кого останавливает. Этими же двумя словами заканчивалось пожелание болельщицы \"Зенита\" болельщикам ЦСКА. Что же она пожелала каждому из них?\n",
            "Answer: Каплю никотина.\n",
            "Comment: Что еще пожелать ненавистным \"коням\"?\n",
            "\n",
            "27\n",
            "Question: Название какого района, расположенного недалеко от деревни СалАрьево, было придумано в жаркие августовские дни 1938 года?\n",
            "Answer: Солнцево.\n",
            "Comment: По одной легенде Саларьево - от итальянского \"Solare\" (солнечный), по другой - августовское солнце навеяло.\n",
            "\n",
            "28\n",
            "Question: \"Сердца ледяные, слезами не растопишь\", - жаловались вынужденные скитаться по деревням и пригородам нищие в рассказе Владимира Сорокина. О ком так говорили странники?\n",
            "Answer: О москвичах.\n",
            "Comment: Москва, как известно, слезам не верит.\n",
            "\n",
            "29\n",
            "Question: Какое слово, по мнению Дейла Карнеги, является самым сладостным и самым важным для каждого человека звуком на любом языке?\n",
            "Answer: его имя\n",
            "\n",
            "30\n",
            "Question: Индейцы апачи считались самыми гордыми и высокомерными среди аборигенов Северной Америки и были уверены в своей непобедимости. Рассказывают, что однажды белый человек спросил апача, неужели нет человека, способного победить апача. Индеец долго думал, а потом ответил: \"Есть. Это - ...\". Кто?\n",
            "Answer: Другой апач.\n",
            "\n",
            "31\n",
            "Question: По одной из версий, это известное в Европе название появилось из-за того, что одним из НИХ воспользовались для отравления монахов. ОНИ описываются в учебниках по химии. Назовите ИХ двумя словами, начинающимися на одну и ту же букву.\n",
            "Answer: Свойства сурьмы.\n",
            "Comment: Согласно одной из версий происхождения английского, а также похожих французского и немецкого названий сурьмы (antimony [Энтимони]), один из монахов обнаружил слабительное действие сурьмы и предложил своим собратьям кашу с сурьмой, в результате чего все монахи отравились и умерли.\n",
            "\n",
            "32\n",
            "Question: По легенде, монахи одного из храмов города БенАрес провинились перед Брахмой, и тот заставил их упражняться с шестьюдесятью четырьмя кольцами. Когда они справятся, миру настанет конец. В названии статьи, в которой приведена эта легенда, фигурирует другой город. Какой?\n",
            "Answer: Ханой.\n",
            "Comment: Эдуард ЛюкА - изобретатель головоломки \"Ханойская башня\" - то ли сам придумал эту легенду, то ли вдохновлялся ею. Тем не менее, название головоломка получила не в честь Бенареса, а в честь другого азиатского города.\n",
            "\n",
            "33\n",
            "Question: Жители одной из местностей на границе Заира и Конго называют находящийся в этой местности водопад \"живой камень\". Почему?\n",
            "Answer: Это водопад Ливингстон.\n",
            "\n",
            "34\n",
            "Question: \"Подобно ребенку в руках кормилицы, феллах мнет и сжимает эту коричневую грудь, чтобы получить бесконечный поток плодородного молока... Никогда еще внимательный сын не ухаживал так за своей старой матерью, которая когда-то дала ему жизнь\". Назовите эту \"мать\" из высказывания Теофиля Готье.\n",
            "Answer: Нил.\n",
            "\n",
            "35\n",
            "Question: \"Кусок от инков - кусок от испанцев\" - такую интересную характеристику дал одному городу в Южной Америке журнал \"Вокруг света\". Назовите этот город.\n",
            "Answer: Куско.\n",
            "\n",
            "36\n",
            "Question: В пьесе Гоголя ОНИ - это Швохнев, Кругель и Утешительный. А Владимир Молчанов, главный редактор \"Своей игры\", кстати, - ОНИ собака мэйл точка ру. Кто ОНИ?\n",
            "Answer: Игроки.\n",
            "Comment: Упомянуты персонажи пьесы Гоголя \"Игроки\". У В. Молчанова именно такой электронный адрес.\n",
            "\n",
            "37\n",
            "Question: \"Когда гремит оружие, музы молчат\". Выражение это является вариантом латинской поговорки, которая известна из речи Цицерона в защиту Милона (52 г. до н.э.), только в этой поговорке молчат не музы, а... Что?\n",
            "Answer: Законы.\n",
            "\n",
            "38\n",
            "Question: В книге \"Трое из навигацкой школы\" группа дворян отправляется на болота на дуэль до первой крови. Не забыв, понятно, запастись солидным запасом спиртного. Едва успели скрестить шпаги да сделать несколько выпадов - как один из дуэлянтов предъявил кровавое пятно на ладони, на чем дуэль закончилась, и начался банкет. А что сделал этот дуэлянт непосредственно перед тем, как предъявил кровь?\n",
            "Answer: Прихлопнул комара.  Убил комара \n",
            "Comment: Лето. Болотистая местность. Комарьё!\n",
            "\n",
            "39\n",
            "Question: Журналист издания \"Главред\", рассказывая о старинных средневековых замках и о том, как они были хорошо защищены, предполагает, что известное устойчивое выражение, скорее всего, происходит с тех времен, а изначально две части этого выражения упоминались в другом порядке. Напишите это выражение в любом варианте.\n",
            "Answer: Мой дом - моя крепость.  Моя крепость - мой дом.\n",
            "Comment: Изначально было \"Моя крепость - мой дом\".\n",
            "\n",
            "40\n",
            "Question: Римляне называли друзей ворами, причём ворующими самое дорогое. Что именно?\n",
            "Answer: Время.\n",
            "\n",
            "41\n",
            "Question: Когда-то, недалеко от Гжели, стоял, производящий изделия из цветной глины, покрытой эмалью, завод братьев Гребенщиковых. Одним из мастеров завода был мастер Антон, прозванный Глина. При этом мастер на кличку не обижался ссылаясь на некоего человека. Я не прошу вас назвать фамилию этого человека. Назовите его имя.\n",
            "Answer: Адам. Из глины и первый человек был сделан.\n",
            "\n",
            "42\n",
            "Question: Удивительно, но фактически это самый большой в Европе заповедник для жизни таких редких животных, как медведь, выдра, барсук, ондатра, рысь, олень, зубр, лошадь Пржевальского. Рассматривается возможность поселения росомахи и амурского тигра. Какое событие привело к возникновению этого заповедника?\n",
            "Answer: Авария на Чернобыльской АЭС.  \n",
            "Comment: Речь идет о зоне отчуждения Чернобыльской АЭС, огромной территории (4000 кв. км), в которой прекращена хозяйственная деятельность человека и отсутствует население. Кроме нескольких сильно загрязненных участков, радиация практически не влияет на экосистему радиационной зоны. Организмы диких животных сами справляются с повышенным фоном, с химическим загрязнением территории и другими негативными факторами.\n",
            "\n",
            "43\n",
            "Question: Георгий Гурджиев считал ЕГО проявлением законов другого мира, Элберт Хаббард определял ЕГО как \"событие, описанное людьми, услышавшими о нём от тех, кто его не видел\". Внимание, вопрос! Какое животное фигурирует в самом малопримечательном из НИХ?\n",
            "Answer: Медведь\n",
            "Comment: \"Обыкновенное ЧУДО\"\n",
            "\n",
            "44\n",
            "Question: Главную свою задачу режиссер фильма \"Анна Каренина\" Сергей Соловьев сформулировал так: \"Я хочу, чтобы молодой зритель моего фильма стал затем...\". Кем?\n",
            "Answer: Читателем романа Анна Каренина.\n",
            "Comment: Т.е. увиденная экранизация должна настолько заинтересовать зрителя, чтобы ему захотелось прочитать первоисточник.\n",
            "\n",
            "45\n",
            "Question: В последнем действии пьесы Евгения Шварца \"Обыкновенное чудо\", когда героям удается избежать печальной развязки, к которой они были так близки, за сценой раздается печальный, постепенно замирающий звук бубенчиков. Хозяин объясняет, что это уезжает на своей белой лошади ОНА. Назовите ЕЕ.\n",
            "Answer: Смерть.\n",
            "Comment: В тексте пьесы: \"Смерть уезжает на своей белой лошади, удирает несолоно хлебавши! Чудо, чудо!\".\n",
            "\n",
            "46\n",
            "Question: В 1980 году в СССР выпустили мультфильм \"Лебеди Непрядвы\", хотя в названии могли фигурировать и другие птицы. К юбилею какого события был приурочен мультфильм?\n",
            "Answer: Куликовской битвы.  Мамаево или Донское побоище.\n",
            "Comment: Куликовская битва произошла 8 сентября 1380 года на Куликовом поле между реками Дон, Непрядва и Красивая Меча. Мультфильм вышел к 600-летию Куликовской битвы.\n",
            "\n",
            "47\n",
            "Question: Одному из персонажей Дэниела Киза, ловкому мошеннику и непревзойденному мастеру побегов, было интересно, сможет ли он убежать, если попадет ТУДА. Герою Бориса Гребенщикова пришлось бы спускаться ТУДА для замены. Замены чего?\n",
            "Answer: Лампочки.\n",
            "Comment: ТУДА - в ад. В песне \"Та, которую я люблю\" \"... в сердце немного света, // Лампочка в тридцать ватт. // Перегорит и эта - // За новой спускаться в ад\".\n",
            "\n",
            "48\n",
            "Question: Бывший шах Ирана, бывший же иранский религиозный диктатор, единственная женщина - премьер-министр Израиля, министр обороны в ее правительстве, китайский \"великий кормчий\". Добавьте к этой веселой компании современного нам Папу Римского и бывшую первую леди США - Жаклин Кеннеди - и попытайтесь определить точное местонахождение некоего Руслана.\n",
            "Answer: Камера в Нарофоминске.\n",
            "Comment: Руслан Халилов - мой сосед по камере...\n",
            "\n",
            "49\n",
            "Question: Суп-рассольник, борщ с пирожком, стерлядь, барашек, заливное из фазана, фрукты в вине, каплуны, салат, спаржа, мороженое. Всё это можно найти на одном из творений художника Аполлинария Васнецова, получившего царский заказ. Но это был вовсе не натюрморт. А что?\n",
            "Answer: Меню.\n",
            "Comment: А конкретно, Васнецову было поручено разрисовать меню торжественного обеда, посвященного коронации Николая II.\n",
            "\n",
            "50\n",
            "Question: В своем завещании он пишет к Октавиану Августу: \"О Горации Флакке не забывай заботиться...\". Назовите его самого.\n",
            "Answer: Меценат.\n",
            "\n",
            "51\n",
            "Question: В книге Юрия Коваля ОН произносит фразы \"Следов на воде не остается\" и \"Озеро похоже по форме на букву О\". Назовите человека, который был ИМ \"Беды\".\n",
            "Answer: Врунгель.\n",
            "Comment: ОН - капитан. Эта его фраза подошла бы Капитану Очевидность.\n",
            "\n",
            "52\n",
            "Question: Академик Янин рассказывает, что городские власти охотно берут у его коллег эту землю - например, насыпать клумбы. Однако чтобы на такой земле росли цветы или трава, ее приходится заражать, то есть смешивать с землей, уже имеющей микробы. Назовите научную специализацию Янина.\n",
            "Answer: Археология.\n",
            "Comment: Сам Янин писал: \"Интересно, что земля, которую мы извлекали из древних слоев в ходе раскопок, - без микробов. Она стерильна. У нас ее охотно берут, чтобы благоустраивать Новгород, например, насыпать клумбы. Мы ее уже проверили и охотно отдаем, поскольку она нам не нужна. Но чтобы на древней земле росли цветы, трава, ее необходимо заражать. Для этого приносят зараженную микробами землю из поверхностного слоя и перемешивают с древним стерильным. И только после такой операции на ней начинает произрастать какая-то растительность.\"\n",
            "\n",
            "53\n",
            "Question: Герой исторического романа заявляет, что Ричард Львиное Сердце видит мир лишь отраженным в НЕМ. Какие два слова, начинающиеся на соседние буквы алфавита, мы обозначили словом \"ОНО\"?\n",
            "Answer: Лезвие меча.\n",
            "Comment: Слишком уж воинственный.\n",
            "\n",
            "54\n",
            "Question: Вблизи канадского города Галифакс находится небольшое кладбище \"Прелестная лужайка\", на котором похоронены 122 человека, погибших в 1 день более 80 лет назад. С начала 1998г. кладбище пользуется огромной популярностью. Особенно много людей навещают могилу N 227. Назовите фамилию мужчины, захороненного в этой могиле.\n",
            "Answer: Доусон. (14-15 апреля 1912г.).\n",
            "\n",
            "55\n",
            "Question: Почему в Уэльсе в XIX в. не получил распространения марксизм?\n",
            "Answer: Валлийский язык очень сложный, и не нашлось переводчиков Капитала на него.\n",
            "\n",
            "56\n",
            "Question: По одной из версий, монах Бертольд Шварц был посажен на бочку с порохом и вознесен взрывом под небеса. Герою романа Пикуля судьба этого монаха напомнила судьбу человека, жившего в XIX веке. Назовите этого человека.\n",
            "Answer: Наполеон Бонапарт.  Наполеон I, Наполеон, Бонапарт.\n",
            "Comment: Бертольд Шварц, как известно, является европейским изобретателем пороха. Наполеон, фактически превративший Европу в \"пороховую бочку\", обречен был сам стать ее жертвой.\n",
            "\n",
            "57\n",
            "Question: Переведите на латынь \"впередсмотрящая\".\n",
            "Answer: Карина\n",
            "\n",
            "58\n",
            "Question: Дуплет.  1. Первое в СССР баночное пиво было выпущено к Московской Олимпиаде. На банке были изображены гербы десятка советских городов. Как называлось это пиво?  2. Один туристический маршрут проходит по территории Новгородской, Ленинградской и Псковской областей. Как называется этот маршрут?\n",
            "Answer: 1. Золотое кольцо России.  2. Серебряное кольцо России.\n",
            "Comment: 1. И олимпийская символика, и русская история.\n",
            "\n",
            "59\n",
            "Question: В романе братьев Стругацких \"Волны гасят ветер\" упоминается, по-видимому, идеальный автомат, изготавливающий перья для письма стилья. По словам Гриши Серосовина, этот автомат изготавливает стилья со скоростью... Со скоростью чего?\n",
            "Answer: Спроса.\n",
            "\n",
            "60\n",
            "Question: Во время высадки в Нормандии Роммель находился в отпуске, а адъютанты боялись Гитлера, поэтому резервные танковые дивизии были выдвинуты лишь к полудню. Какое слово мы пропустили в этом вопросе?\n",
            "Answer: Будить.  Разбудить.\n",
            "Comment: Командование группой армий B [бэ], находившейся во Франции, в 1944 году взял на себя Роммель. Но в день высадки союзников он находился в отпуске в связи с пятидесятилетием жены. Все остальные не имели права отдавать стратегические распоряжения, а окружение Гитлера, опасавшееся, что сообщение о высадке может быть ошибкой, побоялось его разбудить. Проснувшийся между десятью и одиннадцатью утра Гитлер тоже не сразу поверил в случившееся. Поэтому решение о переброске резерва было принято только к полудню, когда основная часть высадки уже была осуществлена.\n",
            "\n",
            "61\n",
            "Question: История из жизни. Мама взяла с собой в кино маленьких детей, и тут к ее ужасу на экране два динозавра схватили человека за руки и разорвали пополам! Однако ее дочка спокойно сказала: \"Посмотри, мамочка, какие они молодцы! Они ДЕЛАЮТ ЭТО, как ты нас учишь\". Какие четыре слова, три из которых начинаются с одной и той же буквы, мы обозначили словами \"ДЕЛАЮТ ЭТО\"?\n",
            "Answer: Делятся друг с другом.\n",
            "\n",
            "62\n",
            "Question: Из \"Советов руководителю\" юмориста Анатолия Мельника: \"Не будь незаменимым, приучай подчиненных делать...\" Что?\n",
            "Answer: ... твою работу.\n",
            "\n",
            "63\n",
            "Question: Название автобуса, в котором находятся диваны, кресла, холодильник, бар, профессиональные звук и свет для дискотеки, а так же караоке, отличается от названия известной венгерской компании на одну букву. Воспроизведите название автобуса.\n",
            "Answer: Шикарус.\n",
            "\n",
            "64\n",
            "Question: На фотографии Алексея Павлова шеренга людей выстроилась перед водонапорной башней. Эта фотография называется \"ИКС и ИГРЕК\". Слова ИКС и ИГРЕК регулярно встречаются в некоторых газетах. Назовите ИКС и ИГРЕК.\n",
            "Answer: Вертикаль и горизонталь.\n",
            "Comment: Без этих слов сложно обойтись в газетах с классическими кроссвордами.\n",
            "\n",
            "65\n",
            "Question: Израильские автоинспекторы полушутя-полусерьезно говорят, что взятку им нужно давать только двумя руками. Почему?\n",
            "Answer: Легче будет застегивать наручники.\n",
            "\n",
            "66\n",
            "Question: Серии фотографий, запечатлевших некие однотипные объекты, дали название из трех слов. Оно представляет собой появившееся в 1970 г. устоявшееся выражение, в котором изменена одна буква. И действительно, многие из этих объектов являются продуктом изящных и красивых инженерных решений. Напишите название серии фотографий.\n",
            "Answer: Мосток - дело тонкое.\n",
            "Comment: Изменено выражение \"Восток - дело тонкое\" из фильма \"Белое солнце пустыни\".\n",
            "\n",
            "67\n",
            "Question: Брак актрисы МХАТа Ольги Книппер и проживавшего в Ялте Чехова Ирина Шкуратова называет ЭТАКИМ. Прототип чеховского героя-ремесленника жил в ТАКОМ переулке. Слово \"ЭТАКИЙ\" длиннее слова \"ТАКОЙ\" на три буквы. Напишите оба слова.\n",
            "Answer: Эпистолярный, столярный.\n",
            "Comment: Переписка Чехова с Книппер составила более восьмисот писем. Прототип Ваньки Жукова, Гавриил Харченко, жил сначала в Таганроге, а потом в Харькове, в Столярном переулке.\n",
            "\n",
            "68\n",
            "Question: Она считалась нечистым местом. Там не вешали икон, а когда туда шли, то снимали с шеи кресты. Тем не менее, посещали ее довольно часто. Что это?\n",
            "Answer: Баня.\n",
            "\n",
            "69\n",
            "Question: Эта рыба семейства лососей весит около 1,5 килограммов и водится в Баренцевом, Белом и Северном морях. Многие невольно упоминали ее, задавая вопрос о победе, ставший неожиданно сложно разрешимым. Назовите эту рыбу.\n",
            "Answer: Горбуша.\n",
            "Comment: Победит ли Гор Буша?\n",
            "\n",
            "70\n",
            "Question: В основе конструкции одной из разновидностей ЭТОГО УСТРОЙСТВА лежит атомайзер, внутри которого расположена нихромовая спираль. Спираль, раскаляясь, превращает ароматизированную жидкость в густой пар. Один картридж для ЭТОГО УСТРОЙСТВА заменяет примерно 5-7... Чего?\n",
            "Answer: Сигарет.\n",
            "Comment: Описано устройство т.н. электронной сигареты.\n",
            "\n",
            "71\n",
            "Question: Статья Анны ТолстОвой посвящена моментам в жизни знаменитого скульптора, не отраженным в биографических трудах о нем. Какими двумя словами она называется?\n",
            "Answer: Неизвестный Неизвестный.\n",
            "Comment: Неизвестные факты об Эрнсте Неизвестном.\n",
            "\n",
            "72\n",
            "Question: ОНА и символ, и оружие, и спортивный инвентарь - причем во многих видах спорта. Не обошлось без НЕЕ и в сельском хозяйстве и транспорте. Известная поговорка говорит о парности у НЕЕ определённого атрибута, другая заявляет о безграничной любви к НЕЙ одного вида животных, и это несмотря на утверждения третьей. Если их пара, то в соединении с овощем ОНИ могут привести к появлению некоего субъекта, который может видоизмениться в существо с определенным количеством неких органов. Вам, наверное, известны и опыты, где ОНА фигурировала наряду с небольшой промысловой рыбой. А кто проводил эти опыты?\n",
            "Answer: Незнайка.\n",
            "Comment: Речь о палке. Палка - жезл, дубинка. Используется в художественной гимнастике, лыжах, бейсболе и т.д. Поговорки: \"Палка о двух концах\", \"Как собака палку любит\", \"Была бы собака, а палка найдется\". Стихотворение из мультфильма: \"Палка, палка, огуречик - вот и вышел человечек, а теперь добавим ножек - получился осьминожек\". Незнайка подобрал к слову \"палка\" рифму \"селедка\".\n",
            "\n",
            "73\n",
            "Question: Последние страницы он писал на смеси ТРЕХ языков - тюркского, персидского и арабского. На его родине его имя стало известным брендом. Назовите это имя.\n",
            "Answer: Афанасий.\n",
            "Comment: Афанасий Никитин, \"Хождение за три моря\".\n",
            "\n",
            "74\n",
            "Question: По словам сотрудника Центрального музея Вооруженных сил Аркадия Дементьева, всего ИХ было около полусотни, но только одно - прикрепленное на восточном фасаде под брюхом коня Вильгельма I - было удостоено чести называться двумя словами. Напишите эти два слова.\n",
            "Answer: Знамя Победы.\n",
            "Comment: [Ведущему: в слове \"Идрицкая\" ударение падает на первый слог.]    На рейхстаге было очень много знамен, но Знаменем Победы стали называть только штурмовой флаг 150-й ордена Кутузова II степени Идрицкой стрелковой дивизии.\n",
            "\n",
            "75\n",
            "Question: Печальная история о том, как на пустынном берегу с НИМИ обошлись самым жестоким образом, была рассказана маленькой девочке двумя братьями-близнецами. По мнению любимого героя Льва Толстого ОНИ помогают человеку оставаться за обедом голодным как можно дольше. А Майку Рачу из Новой Зеландии принадлежит, или по крайней мере принадлежал, мировой рекорд по скорости их открывания: 100 штук за 2 минуты 20 секунд. Кто ОНИ?\n",
            "Answer: Устрицы.\n",
            "Comment: Труляля и Траляля рассказали Алисе стихотворение о том, как Морж и Плотник обманом выманили устриц из моря, а затем съели их. Любимый герой Толстого - Левин из \"Анны Каренины\". Майку Рачу принадлежит мировой рекорд по скорости открывания устриц.\n",
            "\n",
            "76\n",
            "Question: В одной из серий мультфильма \"Госпиталь Хиллтоп\" в госпиталь попадает Волк. Его соседи по палате любили играть с картами и донимать Волка. Какое действие, судя по всему неоднократно выполняемое им раньше, просили сделать Волка?\n",
            "Answer: Сдуть (карточный) домик.  Сдуть, подуть.\n",
            "Comment: Соседи - поросята.\n",
            "\n",
            "77\n",
            "Question: Как известно, одним из популярнейших домашних богов славян был Чур. А как назывался священный столб, воспринимавшийся как знак Симаргла, в котором таился живой огонь?\n",
            "Answer: Чурбан.\n",
            "\n",
            "78\n",
            "Question: Это упоминается в названии серии встреч между \"Звездами\" и \"Клинками\" в 1999 году. А за 15 лет до этого, в 1984 году, началась другая серия, где это также упоминается. Вторым был Джек Шолдер, третьим - Чак Рассел, четвертым - Ренни Харлин, пятым - Стивен Хопкинс, шестым - Брэкин Мейер. Назовите того, кто был и первым и седьмым.\n",
            "Answer: Уэс Крэйвен.\n",
            "Comment: Режиссер 1-й и 7-й частей \"Кошмара на улице Вязов\", 1999 год - Финал кошмаров в НХЛ.\n",
            "\n",
            "79\n",
            "Question: Маленький сын Памелы Андерсон предполагал, что мама просто забыла прорезать дырку на нижний этаж. Какое увлечение Памелы стало причиной такого вывода?\n",
            "Answer: Танцы на шесте.  Стриптиз, стрип-дэнс, танец на пилоне, пилонный танец, pole dance, шестовая акробатика, .\n",
            "Comment: Дома у экс-модели \"Плэйбоя\" сохранился шест для стриптиза и других упражнений. Сын думал, что это пожарный шест, и мама просто забыла прорезать в полу отверстие, чтобы спускаться на первый этаж.\n",
            "\n",
            "80\n",
            "Question: Хазары.  1. Эта религия была господствующей в Хазарском каганате.  2. Таково имя верховного хазарского божества доиудаистического периода истории.  3. Из этого рода происходили все хазарские каганы.  4. В письме к этому испанскому иудею каган Иосиф изложил историю Хазарии.  5. Этот человек в начале IX века произвел в Хазарии государственный переворот и превратил иудаизм в государственную религию каганата.\n",
            "Answer: 1. Иудаизм.  2. Тенгри-хан.  3. Ашина.  4. Хасдай Ибн-Шафрут.  5. Обадия.\n",
            "\n",
            "81\n",
            "Question: Она появилась в Париже за 6 лет до Эйфелевой башни. Ее создатель предпочел укрыться под псевдонимом \"Н.Клаус из Сиама, мандарин колледжа Ли Су Цян\". А столица какой страны дала ей название?\n",
            "Answer: Вьетнама\n",
            "Comment: Речь идет о головоломке \"Ханойская башня\".\n",
            "\n",
            "82\n",
            "Question: По остроумному замечанию Владимира Борисова, казино - это такое место, где и простой человек, и миллиардер могут, по воле случая, легко стать ИМ. ОН присутствует в названии телепередачи. Назовите ЕГО.\n",
            "Answer: Миллионер.\n",
            "Comment: Передача - \"Кто хочет стать миллионером?\"; в казино простой человек может разбогатеть, а миллиардер - разориться.\n",
            "\n",
            "83\n",
            "Question: [Ведущему: читать \"мельОныкы\"!]  В первой половине XX века львовские модники носили \"мельоники\" - выпуклые шляпы с очень узкими полями, напоминавшие половинку АЛЬФЫ. Назовите АЛЬФУ.\n",
            "Answer: Дыня.\n",
            "Comment: По-польски дыня \"melon\" - \"мЕльон\"; впрочем, и на английское \"melon\" похоже. :-)\n",
            "\n",
            "84\n",
            "Question: Отечественная редакция \"Сербской Александрии\", средневекового романа о подвигах Александра Македонского, содержит информацию о том, как Александр повелел своему воеводе Селевку \"с тысящью тысящ воинства... в некое место съкрытися\". Или, например, о том, что Александр \"Антиоха мниха воеводой вместо себя... на царьском престоле посадил, а сам как один из подчиненных Антиоху предстоял\". Историк Андрей Петров утверждает, что именно \"Сербская Александрия\" повлияла на произведение, посвященное известному событию. Назовите это событие.\n",
            "Answer: Куликовская битва.\n",
            "Comment: Эпизоды вылазки Засадного полка и обмена доспехами между Дмитрием Донским и Михаилом Бренком, видимо, появились в \"Сказании о Мамаевом побоище\" под влиянием \"Сербской Александрии\".\n",
            "\n",
            "85\n",
            "Question: Статья, рассказывающая о падении рынка оплаченных, неперсонализированных коммуникаций в 2009 году и его росте в 2010 году, называется \"Конец ЕЕ\". Назовите человека, которого Сергей Бородин назвал непревзойденным объявителем ЕЕ.\n",
            "Answer: Леонид Якубович.\n",
            "Comment: ОНА - рекламная пауза.\n",
            "\n",
            "86\n",
            "Question: Павел Федотов, картина \"Старик у окна\". Вечер. Задремавший у окна старик. На подоконнике - пустая бутыль из-под вина, на столе - ни крошки съестного. Что же в таком случае грызут две мыши, взобравшиеся на стол?\n",
            "Answer: Свечи.\n",
            "\n",
            "87\n",
            "Question: \"От того какую музыку слушают и любят люди, зависят переживания их души в астральных сферах\". Именно так Борис Гребенщиков трактует русскую пословицу. Какую?\n",
            "Answer: Каков поп, таков и приход.\n",
            "Comment: Слово \"поп\" в пословице БГ истолковал как \"популярная музыка\", а \"приход\" - как действие наркотика.\n",
            "\n",
            "88\n",
            "Question: Вы наверняка помните, как носительницу этого редкого женского имени постигло несчастье. Одна из производных форм этого имени созвучна с тем, чему обучали некоего заведующего. Назовите растение, упоминавшееся преподавателем в процессе этого обучения.\n",
            "Answer: Редиска\n",
            "Comment: Имя - Федора, одна из уменьшительных форм этого имени - Феня. Блатной фене обучали заведующего детсадом Трошкина из фильма \"Джентльмены удачи\".\n",
            "\n",
            "89\n",
            "Question: Измененная цитата: \"Если ваш ужин оказался плох, отложите моего ИКСА и перелистайте шедевры Тиссо о воздержании...\". Кому принадлежит эта цитата?\n",
            "Answer: Бомарше.\n",
            "Comment: У Пушкина Бомарше советовал Сальери перечесть \"Женитьбу Фигаро\", а вовсе не откладывать ее. На ИКСА мы заменили \"Севильского цирюльника\".\n",
            "\n",
            "90\n",
            "Question: В советской мультипликационной экранизации в подчинении у ИКСОВ были свои собственные, так сказать, \"ИКСЫ второго порядка\", говорящие еще более тонкими голосами. Назовите ИКСОВ.\n",
            "Answer: Лилипуты.\n",
            "Comment: В этой экранизации - мультфильме \"Новый Гулливер\" - лилипуты отличились не только малым размером, но и неестественно тонкими голосами. \"Лилипуты второго порядка\" настолько жестоко притеснялись обычными лилипутами, что попавший к ним пионер устроил революцию.\n",
            "\n",
            "91\n",
            "Question: Этих трех сестричек звали Элси, Лэси и Тилси. Жили они на дне колодца, питались исключительно киселем... А какой особенностью обладали их рисунки?\n",
            "Answer: Они рисовали мышеловки, месяц, математику, множество - все, что начинается на М.\n",
            "Comment: \"Алиса в Стране чудес\".\n",
            "\n",
            "92\n",
            "Question: Ромен Гари пишет, как, возвращаясь с фронта с победой, ехал домой с орденами, медалями, капитанскими нашивками и с небольшим количеством ЕГО, как раз таким, сколько надо для придания веса. Назовите ЕГО точно.\n",
            "Answer: Свинец.  Металл.\n",
            "\n",
            "93\n",
            "Question: Помните ли вы церемонию вручения Оскаров 2002 года? Впрочем, неважно. Итак, внимание, вопрос!  Редакторы американского издания книг о Гарри Поттере СДЕЛАЛИ ЭТО с двумя второстепенными персонажами произведения. Назовите того, кто, по мнению многих журналистов, пытался сделать с собой обратное, если результат выглядит не особо удачным.\n",
            "Answer: Майкл Джексон.\n",
            "Comment: Редакторы американского издания сделали нескольких героев чернокожими из соображений политкорректности.\n",
            "\n",
            "94\n",
            "Question: Утверждают, что служивший на флоте Николай Рубцов был самым субтильным в команде. Какой глагол использует Николай Коняев, когда рассказывает, как Рубцов надел выданную ему робу?\n",
            "Answer: Утонул.\n",
            "Comment: Удачный глагол для морской тематики.\n",
            "\n",
            "95\n",
            "Question: На государственном гербе Индонезии изображены пять символов страны, знаменующие пять принципов жизни индонезийцев. Так, голова буйвола символизирует суверенитет, звезда - религию, дерево - самосознание. А что символизируют колос риса и кольцо из цепи?\n",
            "Answer: Благосостояние и равенство.\n",
            "\n",
            "96\n",
            "Question: Дуплет.  1. Рекламная рассылка авиакомпании \"Ryanair\" [райанэйр], пришедшая по электронной почте в начале ноября 2016 года, уведомляла о распродаже билетов по весьма низким ценам и утверждала, что данное предложение не сможет удалить даже ЭТОТ ЧЕЛОВЕК. Назовите имя ЭТОГО ЧЕЛОВЕКА.  2. Рекламная рассылка авиакомпании \"Ryanair\" [райанэйр], пришедшая по электронной почте в начале ноября 2016 года, уведомляла о распродаже билетов по весьма низким ценам и утверждала, что данное предложение не сможет быть побито никаким козырем. Воспроизведите английский глагол, употребленный в оригинальном сообщении.\n",
            "Answer: 1. Хиллари.  2. Trump.\n",
            "Comment: (pic: 20170110.jpg)    Trump - по-английски \"козырь\" (если существительное), to trump - \"бить козырем\", \"превзойти\". Для любителей бриджа: NT - No Trumps - без козырей.\n",
            "\n",
            "97\n",
            "Question: Моток перепутанных верёвок со множеством узелков, раковины, нанизанные на шнурки, дым костра, гул барабанов. А что мы сейчас используем вместо этого?\n",
            "Answer: Письма, например.\n",
            "\n",
            "98\n",
            "Question: Отрывок из стихотворения Сергея Алымова о товарище Сталине, написанного в 1951 году:  На большой земле Китая,  Дорогое всем сердцам,  Это имя возвещает  [пропуск] - мир, конец - [пропуск]?  Какие два слова мы пропустили?\n",
            "Answer: ... фанзам..., ... дворцам.\n",
            "\n",
            "99\n",
            "Question: На граффИти в Тегеране ОНИ представляют собой траекторию падающих бомб. С 1796 по 1818 год ИХ было пятнадцать. Назовите ИХ.\n",
            "Answer: Полосы на флаге США.\n",
            "Comment: Сперва планировалось, что будет увеличиваться не только число звезд, но и число полос на американском флаге.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i in range(100):\n",
        "    print(i)\n",
        "    print_dict(raw_data['test'][i])\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1 </span>inputs = tokenizer(                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>generate_prompt_infer(quest),                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>return_tensors=<span style=\"color: #808000; text-decoration-color: #808000\">\"pt\"</span>,                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># truncation=True,</span>                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># max_length=CUTOFF_LEN_TEST,</span>                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'quest'</span> is not defined\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
              "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m2\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1 \u001b[0minputs = tokenizer(                                                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2 \u001b[2m│   │   \u001b[0mgenerate_prompt_infer(quest),                                                        \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m3 \u001b[0m\u001b[2m│   │   \u001b[0mreturn_tensors=\u001b[33m\"\u001b[0m\u001b[33mpt\u001b[0m\u001b[33m\"\u001b[0m,                                                                 \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m4 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# truncation=True,\u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m5 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# max_length=CUTOFF_LEN_TEST,\u001b[0m                                                        \u001b[31m│\u001b[0m\n",
              "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
              "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'quest'\u001b[0m is not defined\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "inputs = tokenizer(\n",
        "        generate_prompt_infer(quest),\n",
        "        return_tensors=\"pt\",\n",
        "        # truncation=True,\n",
        "        # max_length=CUTOFF_LEN_TEST,\n",
        "    )\n",
        "inputs[\"input_ids\"].size(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 <span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(inputs[<span style=\"color: #808000; text-decoration-color: #808000\">'input_ids'</span>][<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>])                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'inputs'</span> is not defined\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
              "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1 \u001b[96mlen\u001b[0m(inputs[\u001b[33m'\u001b[0m\u001b[33minput_ids\u001b[0m\u001b[33m'\u001b[0m][\u001b[94m0\u001b[0m])                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
              "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'inputs'\u001b[0m is not defined\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "len(inputs['input_ids'][0])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "083dc60725f34ef698730f26c9086dcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0de70903e0134f0e83dc678521357e69",
            "placeholder": "​",
            "style": "IPY_MODEL_15017ff480f24f62946bda435cd51102",
            "value": "Map: 100%"
          }
        },
        "09a2032426964f9db2bb0dc724d2a547": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0de70903e0134f0e83dc678521357e69": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f43b4ac5ceb4b8a80fab468915e3097": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10cd244eb0cd40b6b1e29b395fd19ddf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "15017ff480f24f62946bda435cd51102": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "173cd90b7aae4947ba95965a646a283b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "19ace52302024c2482cf87216777870b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c6ccd8b8eaf42b08fb330f7b852146d",
            "placeholder": "​",
            "style": "IPY_MODEL_42ced876a2544d60b13311a471da27ca",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "1fe458cef4714a90966bf77d70720a9b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "214b4872377d4e75b0d063788463b757": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "222150732edf4bfa8b838c162ce541bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90ba8a643b6c467dbcef702766424845",
            "placeholder": "​",
            "style": "IPY_MODEL_173cd90b7aae4947ba95965a646a283b",
            "value": " 33/33 [00:17&lt;00:00,  1.82it/s]"
          }
        },
        "23ae4d80693d4e9ca9fa84ec04582fe0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_083dc60725f34ef698730f26c9086dcb",
              "IPY_MODEL_f0452141712c4d8ea206b70fb5df6d75",
              "IPY_MODEL_acdf3dc9176645108458e29f66a35708"
            ],
            "layout": "IPY_MODEL_303c086f27154250ace62e34947d8ca6"
          }
        },
        "28c83c6d696d4fa794fc28bb834699e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "29112857345a49fc999436fdafa507e6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2975c4c32ac949c9ba538a8e5f99c441": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "303c086f27154250ace62e34947d8ca6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "33e6e93ed60e42b1a0f8341db5d2fedc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "352276c412234087821381d50daa52e9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3791e05934764ceea2852df0ac9a7491": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a2119497387439ab42cf52bfe7e0fe6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3a59689840c84f7fa5b0140bd7158ea4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cda2c22abdb4434e972037597c33733d",
            "placeholder": "​",
            "style": "IPY_MODEL_10cd244eb0cd40b6b1e29b395fd19ddf",
            "value": " 1/1 [00:00&lt;00:00, 24.14it/s]"
          }
        },
        "404c6bfeb19f477a8ffec70706087e6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b92c8a8222bc40248724de6f6a22f264",
            "placeholder": "​",
            "style": "IPY_MODEL_3a2119497387439ab42cf52bfe7e0fe6",
            "value": " 0/2 [00:00&lt;?, ?it/s]"
          }
        },
        "4092aaf86bdf43acbc7666d18ac876bd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42675d2bd1024972a5dadbfdf8e84712": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42ced876a2544d60b13311a471da27ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "49bcc6e230dc415eac3bd91a833f83a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b490fd961ee3469bbed398c05d91ef9a",
            "placeholder": "​",
            "style": "IPY_MODEL_d5e8123382354f638367f259bb592413",
            "value": " 1/1 [00:00&lt;00:00, 39.65it/s]"
          }
        },
        "4a623259049d467fa65d714fd0318049": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e961764ce7b64a539a22dd677007e2e9",
              "IPY_MODEL_cc67c1d69fa84d82bbee94b08c8dab5c",
              "IPY_MODEL_ec7aec0c42374db895dde5819c2a143d"
            ],
            "layout": "IPY_MODEL_29112857345a49fc999436fdafa507e6"
          }
        },
        "4daed62d06794dccba5836a09802c438": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6db4af7551fe448880eab86e0c0a236c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_28c83c6d696d4fa794fc28bb834699e1",
            "value": 1
          }
        },
        "5bdae2da2843478596b286c3cb1ffbcb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d2cea68374346afa3c49e04aa4e9481": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "62c966d78b12433b9258a023602d709e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6db4af7551fe448880eab86e0c0a236c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6dcc2d3a7ac64aef89a514690ef0afec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "705bcb43af2e4ca084cf5c5a8cd7d989": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "73b824cc12d74f4eae7c408b61267539": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b7539f3ae8449608a1fdb9e11f96ce0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c1be2d7be30401d99abc3158f413130": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_19ace52302024c2482cf87216777870b",
              "IPY_MODEL_88367badce964703ae7e0d86d080e315",
              "IPY_MODEL_222150732edf4bfa8b838c162ce541bb"
            ],
            "layout": "IPY_MODEL_e08aa6c091444c35baa84fa1c7d1f57d"
          }
        },
        "7c6ccd8b8eaf42b08fb330f7b852146d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c91c0df02a4461590af36def5789499": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d070b0d60a14d44ad1ed94b20ff02ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42675d2bd1024972a5dadbfdf8e84712",
            "max": 370,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_09a2032426964f9db2bb0dc724d2a547",
            "value": 370
          }
        },
        "7f04e58038e64192a0ad1058c2111675": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84402759dc7a45cdb7ab729a36983d39": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88367badce964703ae7e0d86d080e315": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3791e05934764ceea2852df0ac9a7491",
            "max": 33,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dce17622d76347d38bb7e3991d768629",
            "value": 33
          }
        },
        "8972cf3fb361445db3d798b734055d78": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8bb67fbe2a3e4357b5c72b6c2e1b37db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8c0b2ebd6c63498fadb16e1c29c0ee03": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b067fd4b35724cbc8791ff4548fe46da",
              "IPY_MODEL_c5e728cf48664d3a83348fad1605983a",
              "IPY_MODEL_404c6bfeb19f477a8ffec70706087e6c"
            ],
            "layout": "IPY_MODEL_0f43b4ac5ceb4b8a80fab468915e3097"
          }
        },
        "8d5b33aa97c94f6796ede0a59e450557": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fbb61c48949e4f67b57adaae91dda0ef",
            "placeholder": "​",
            "style": "IPY_MODEL_7c91c0df02a4461590af36def5789499",
            "value": "100%"
          }
        },
        "90ba8a643b6c467dbcef702766424845": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4448402524e4b8e880b83d0c0dc5aa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8d5b33aa97c94f6796ede0a59e450557",
              "IPY_MODEL_4daed62d06794dccba5836a09802c438",
              "IPY_MODEL_49bcc6e230dc415eac3bd91a833f83a8"
            ],
            "layout": "IPY_MODEL_dac25037ef6043eba05479906e461bed"
          }
        },
        "a47788f290e94958bfb2086f37c5518d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa1ce2ba4d154c8299f73a7cde2b1586": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d76b2d905dbf45f7941d2785e6e6335e",
            "placeholder": "​",
            "style": "IPY_MODEL_73b824cc12d74f4eae7c408b61267539",
            "value": "100%"
          }
        },
        "acdf3dc9176645108458e29f66a35708": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62c966d78b12433b9258a023602d709e",
            "placeholder": "​",
            "style": "IPY_MODEL_8972cf3fb361445db3d798b734055d78",
            "value": " 52000/52002 [01:17&lt;00:00, 541.19 examples/s]"
          }
        },
        "b067fd4b35724cbc8791ff4548fe46da": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f04e58038e64192a0ad1058c2111675",
            "placeholder": "​",
            "style": "IPY_MODEL_5d2cea68374346afa3c49e04aa4e9481",
            "value": "Loading checkpoint shards:   0%"
          }
        },
        "b490fd961ee3469bbed398c05d91ef9a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b92c8a8222bc40248724de6f6a22f264": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5e728cf48664d3a83348fad1605983a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dfa5e05bd7974455931c639766acfd9a",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8bb67fbe2a3e4357b5c72b6c2e1b37db",
            "value": 0
          }
        },
        "caf150ece0a54bf0b8ca93fd78dd6447": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cc67c1d69fa84d82bbee94b08c8dab5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed875ee7a80b445f9af65c7f063d46a7",
            "max": 8434381,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_caf150ece0a54bf0b8ca93fd78dd6447",
            "value": 8434381
          }
        },
        "cda2c22abdb4434e972037597c33733d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cec86d69d0c84d47940f208324097677": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5bdae2da2843478596b286c3cb1ffbcb",
            "placeholder": "​",
            "style": "IPY_MODEL_a47788f290e94958bfb2086f37c5518d",
            "value": "Downloading (…)/adapter_config.json: 100%"
          }
        },
        "d5e8123382354f638367f259bb592413": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d6b1b93d46204c85a1119983e02b5b1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d76b2d905dbf45f7941d2785e6e6335e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dac25037ef6043eba05479906e461bed": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbc82ac1e7034b0a8787158c9cd01501": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b7539f3ae8449608a1fdb9e11f96ce0",
            "placeholder": "​",
            "style": "IPY_MODEL_214b4872377d4e75b0d063788463b757",
            "value": " 370/370 [00:00&lt;00:00, 21.1kB/s]"
          }
        },
        "dce17622d76347d38bb7e3991d768629": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dfa5e05bd7974455931c639766acfd9a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e08aa6c091444c35baa84fa1c7d1f57d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e961764ce7b64a539a22dd677007e2e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1fe458cef4714a90966bf77d70720a9b",
            "placeholder": "​",
            "style": "IPY_MODEL_d6b1b93d46204c85a1119983e02b5b1f",
            "value": "Downloading adapter_model.bin: 100%"
          }
        },
        "eb15b5ce66f74cfa9402e95764a99ed9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cec86d69d0c84d47940f208324097677",
              "IPY_MODEL_7d070b0d60a14d44ad1ed94b20ff02ea",
              "IPY_MODEL_dbc82ac1e7034b0a8787158c9cd01501"
            ],
            "layout": "IPY_MODEL_4092aaf86bdf43acbc7666d18ac876bd"
          }
        },
        "ec7aec0c42374db895dde5819c2a143d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84402759dc7a45cdb7ab729a36983d39",
            "placeholder": "​",
            "style": "IPY_MODEL_33e6e93ed60e42b1a0f8341db5d2fedc",
            "value": " 8.43M/8.43M [00:00&lt;00:00, 56.9MB/s]"
          }
        },
        "ed875ee7a80b445f9af65c7f063d46a7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0452141712c4d8ea206b70fb5df6d75": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_352276c412234087821381d50daa52e9",
            "max": 52002,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2975c4c32ac949c9ba538a8e5f99c441",
            "value": 52002
          }
        },
        "fabf45c2a7144c2b8efb38259c2b2828": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aa1ce2ba4d154c8299f73a7cde2b1586",
              "IPY_MODEL_fe53b0107eb14f07b51cfd6e3ea6276d",
              "IPY_MODEL_3a59689840c84f7fa5b0140bd7158ea4"
            ],
            "layout": "IPY_MODEL_6dcc2d3a7ac64aef89a514690ef0afec"
          }
        },
        "fbb61c48949e4f67b57adaae91dda0ef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcafee8c2eb54289b91909fc7881a945": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe53b0107eb14f07b51cfd6e3ea6276d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fcafee8c2eb54289b91909fc7881a945",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_705bcb43af2e4ca084cf5c5a8cd7d989",
            "value": 1
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
